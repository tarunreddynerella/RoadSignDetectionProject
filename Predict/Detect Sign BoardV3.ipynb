{"cells":[{"cell_type":"code","source":["try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    import zipfile\n","    with zipfile.ZipFile('/content/drive/MyDrive/DL Project/DataSet1.zip', 'r') as zip_ref:\n","        zip_ref.extractall('./DataSet1')\n","except:\n","    print(\"Using Local Machine\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L3U3WSH_p4Qw","executionInfo":{"status":"ok","timestamp":1682574435101,"user_tz":360,"elapsed":3831,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"}},"outputId":"8ce82117-8120-44e9-9036-02a48f8d984a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Using Local Machine\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"33EE-Og9MgYN","outputId":"903ea404-df8f-4896-b144-1cd69ef497f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'yolov5'...\n","remote: Enumerating objects: 15598, done.\u001b[K\n","remote: Counting objects: 100% (205/205), done.\u001b[K\n","remote: Compressing objects: 100% (148/148), done.\u001b[K\n","remote: Total 15598 (delta 98), reused 119 (delta 57), pack-reused 15393\u001b[K\n","Receiving objects: 100% (15598/15598), 14.64 MiB | 21.26 MiB/s, done.\n"]}],"source":["!git clone https://github.com/ultralytics/yolov5.git\n","!pip install -r yolov5/requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YH9O9SKQMgYO"},"outputs":[],"source":["# Include all packages\n","import os\n","import gc\n","import cv2\n","import shutil\n","import numpy as np\n","import pandas as pd\n","from time import time\n","from datetime import datetime\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","from yolov5.models.yolo import Model\n","from sklearn.model_selection import train_test_split\n","\n","from pycocotools.coco import COCO\n","from pycocotools.cocoeval import COCOeval\n","\n","import torchvision\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","from copy import deepcopy\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-VC_coAuMgYO"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RIo8lRlaMgYP"},"outputs":[],"source":["def CannyEdge(capturedImage):\n","    grayScale = cv2.cvtColor(capturedImage, cv2.COLOR_BGR2GRAY)\n","    constrastKernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5) )\n","    topHat = cv2.morphologyEx(grayScale, cv2.MORPH_TOPHAT, constrastKernel)\n","    blackHat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, constrastKernel)\n","    grayScale = grayScale + topHat - blackHat\n","    bilateralFilter = cv2.bilateralFilter(grayScale, 11, 17, 17)\n","    imageMedian = np.median(capturedImage)\n","    lowerThreshold = max(0, (0.7 * imageMedian))\n","    upperThreshold = min(255, (0.7 * imageMedian))\n","    cannyEdgeImage = cv2.Canny(bilateralFilter, lowerThreshold, upperThreshold)\n","    cannyEdgeImage = cv2.bitwise_not(cannyEdgeImage)\n","    cannyEdgeImage = cv2.cvtColor(cannyEdgeImage, cv2.COLOR_GRAY2BGR)\n","    return cannyEdgeImage"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w_qSITrUMgYP"},"outputs":[],"source":["def ResizeImage(image: np.ndarray, x1: int, y1: int, x2: int, y2: int, newWidth: int, newHeight: int) -> tuple:\n","    originalHeight, originalWidth = image.shape[:2]\n","    widthScale = newWidth / originalWidth\n","    heightScale = newHeight / originalHeight\n","    resizedImage = cv2.resize(\n","        image, (newWidth, newHeight), interpolation=cv2.INTER_LINEAR)\n","    x1New, y1New = int(x1 * widthScale), int(y1 * heightScale)\n","    x2New, y2New = int(x2 * widthScale), int(y2 * heightScale)\n","    return resizedImage, x1New, y1New, x2New, y2New\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DzSrOjv7MgYP"},"outputs":[],"source":["def LoadDataSet(dataSetFolderPath: str) -> tuple:\n","    images = []\n","    annotations = []\n","    annotationsFilePath = dataSetFolderPath+\"/annotations.csv\"\n","    annotationsDataFrame = pd.read_csv(annotationsFilePath, sep=\",\")\n","    uniqueSigns = annotationsDataFrame['class'].unique().tolist()\n","    for index, row in annotationsDataFrame[1:].iterrows():\n","        image = cv2.imread(dataSetFolderPath+\"/\"+row[0])\n","        images.append(image)\n","        annotations.append(\n","            [uniqueSigns.index(row[5]), row[1], row[2], row[3], row[4]])\n","\n","    del annotationsDataFrame\n","\n","    return images, annotations, len(uniqueSigns)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nj-6l1b_MgYQ"},"outputs":[],"source":["def PreProcessDataSet(images: list, annotations: list, batchSize: int, resize: tuple) -> tuple:\n","    resizedImages = [[] for i in range(47)]\n","    newAnnotations = [[] for i in range(47)]\n","    for i, image in enumerate(images):\n","        [classIndex, x1, y1, x2, y2] = annotations[i]\n","        resizedImage, x1New, y1New, x2New, y2New = ResizeImage(\n","            image, x1, y1, x2, y2, resize[0], resize[1])\n","         # xCenter = ((x1New + x2New) / 2) / resize[0]\n","        # yCenter = ((y1New + y2New) / 2) /  resize[1]\n","        # width = (x2New - x1New) /  resize[0]\n","        # height = (y2New - y1New) /  resize[1]\n","        # newAnnotations[classIndex].append(\n","        #     [xCenter, yCenter, width, height])\n","\n","        # resizedImage = CannyEdge(resizedImage)\n","        resizedImages[classIndex].append(resizedImage)\n","        newAnnotations[classIndex].append(\n","            [x1New, y1New, x2New, y2New])\n","    # AllX_train=[]\n","    # AllX_val = []\n","    # Ally_train = [] \n","    # Ally_val = []\n","    # for i in range(47):\n","    #     X_train, X_val, y_train, y_val = train_test_split(\n","    #         resizedImages[i], newAnnotations[i], test_size=0.1, random_state=42)\n","    #     AllX_train.append(X_train)\n","    #     AllX_val.append(X_val)\n","    #     Ally_train.append(y_train) \n","    #     Ally_val.append(y_val)\n","\n","    # return AllX_train, AllX_val, Ally_train, Ally_val\n","    return resizedImages, newAnnotations\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VYcGr92mMgYQ"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, data, transform=None):\n","        self.data = data\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        inputData, label = self.data[idx]\n","\n","        if self.transform:\n","            inputData = self.transform(inputData)\n","        inputData = torch.from_numpy(inputData).float()\n","        label = torch.tensor(label).float()\n","        return inputData, label\n","\n","def CreateDataLoaders(X_train, X_val, y_train, y_val, batchSize):\n","    trainDataSet = []\n","    valDataSet = []\n","    for i in range(len(X_train)):\n","        trainDataSet.append((X_train[i], y_train[i]))\n","\n","    for i in range(len(X_val)):\n","        valDataSet.append((X_val[i], y_val[i]))\n","\n","    trainDataSet = CustomDataset(trainDataSet)\n","    valDataSet = CustomDataset(valDataSet)\n","    trainDataLoader = DataLoader(\n","        trainDataSet, batch_size=batchSize, shuffle=True, num_workers=4)\n","    valDataLoader = DataLoader(\n","        valDataSet, batch_size=batchSize, shuffle=False, num_workers=4)\n","\n","    return trainDataLoader, valDataLoader\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xhQjQ-UQMgYQ"},"outputs":[],"source":["def CreateYolov5Model(numClasses: int, version: str):\n","    congfigFile = \"yolov5/models/yolov5{}.yaml\".format(version)\n","    model = Model(congfigFile, ch=3, nc=numClasses)\n","    # model.load_state_dict(torch.load(\"yolov5{}.pt\".format(version))[\"model\"].state_dict(), strict=False)\n","\n","    return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rkm--g-BMgYR"},"outputs":[],"source":["def DetectImage(model, inputs, device, conf_thres=0.2, iou_thres=0.5):\n","    model.eval()\n","\n","    inputs = torch.tensor(inputs, dtype=torch.float32)\n","    inputs = inputs.unsqueeze(0)\n","    inputs = inputs.permute(0, 3, 1, 2)\n","    inputs = inputs.to(device)\n","    conf_thres = torch.tensor(conf_thres)\n","    with torch.no_grad():\n","        output = model(inputs)\n","        # max_conf_obj_idx = torch.argmax(output[0][..., 4:5], dim=1)\n","        # output = output[0][torch.arange(output[0].size(0)), max_conf_obj_idx]\n","        # output = torchvision.ops.nms(output, conf_thres, iou_thres)\n","        # max_conf_obj_idx = torch.argmax(output[0][..., 4:5], dim=1)\n","        # output = output[0][torch.arange(output[0].size(0)), max_conf_obj_idx]\n","        output = output[0]\n","        box_coordinates = output[..., :4].view(-1, 4)\n","        confidence_scores = output[..., 4].view(-1)\n","        nms_indices = torchvision.ops.nms(box_coordinates, confidence_scores, iou_thres)\n","        output = output.view(-1, output.shape[-1])[nms_indices]\n","    # Remove the batch dimension\n","    output = output.squeeze(0)\n","    return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MbrCcAaEMgYR"},"outputs":[],"source":["batchSize = 32\n","inputShape = (640, 640)\n","epochs = 100\n","numAnchors = 3\n","yolo5Version = 'm'\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yo5o2tQ0MgYR"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gDKfTcenMgYR"},"outputs":[],"source":["images, annotations, numClasses = LoadDataSet(\"./DataSet1\")\n","# numClasses = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nGS-xGAtMgYS"},"outputs":[],"source":["X_train, y_train = PreProcessDataSet(\n","    images, annotations, batchSize, inputShape)\n","X_val = [[] for i in range(47)]\n","y_val = [[] for i in range(47)]\n","del images\n","del annotations\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wqAMPV0qMgYS"},"outputs":[],"source":["# trainDataLoader, valDataLoader = CreateDataLoaders(\n","#     X_train, X_val, y_train, y_val, batchSize)\n","# del X_train\n","# del y_train\n","# del X_val\n","# del y_val\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-1xrW3kmMgYS"},"outputs":[],"source":["yolov5Model = CreateYolov5Model(numClasses,yolo5Version)\n","yolov5Model = yolov5Model.to(device)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"STybO-FoMgYS"},"outputs":[],"source":["savedModels = [ modelName for modelName in os.listdir('/content/drive/MyDrive/DL Project/Trained Models/') if ('yolov5SingleModelv2' in modelName)]\n","savedModels.sort()\n","print(savedModels[-1])\n","yolov5Model.load_state_dict(torch.load('/content/drive/MyDrive/DL Project/Trained Models/'+savedModels[-1], map_location=torch.device(device)))\n"]},{"cell_type":"code","source":[],"metadata":{"id":"-0xOOD_S9Vam"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"dmHwhORzRVUP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMoAlEgdMgYS"},"outputs":[],"source":["import random\n","randInt = random.randint(0,len(X_val))\n","\n","image = X_val[randInt]\n","image1 = deepcopy(image)\n","# try:\n","#     from google.colab.patches import cv2_imshow\n","#     cv2_imshow(image)\n","# except:\n","#     print(\"using Local\")\n","#     cv2.imshow(\"Input Image\", image)\n","\n","predictions = DetectImage(yolov5Model, image, device)\n","[a1,b1,a2,b2] = y_val[randInt]\n","bBoxs = [[a1,b1,a2,b2]]\n","machingbBoxes = []\n","albBoxs =[]\n","i=0\n","for pred in predictions:\n","    \n","    i+=1\n","    # print(predictions.shape)\n","    # for p in pred:\n","    #   print(int(p))\n","    # break\n","    x1, y1, x2, y2, m1,m2 = pred[:6]\n","    # x2 = x1+x2\n","    # y2 = y1+y2\n","    # albBoxs.append( [x1, y1, x1+x2, y2])\n","    m1,m2, x1, y1, x2, y2= int(m1), int(m2),int(x1), int(y1), int(x2), int(y2)\n","    if(a1 == x1 or a2 == x2 or b1 == y1 or b2 == y2 ):\n","      if(((x1-x2) >= 17 and (x1-x2) <= 32) and ((y1-y2) >= 31 and (y1-y2)<= 56) ):\n","        machingbBoxes.append([x1, y1, x2,y2])\n","      \n","    if(((x1-x2) >= 17 and (x1-x2) <= 32) and ((y1-y2) >= 31 and (y1-y2)<= 56) ):\n","        bBoxs.append([x1, y1, x2, y2])\n","    # if(((x2) >= 17 and (x2) <= 44) and ((y2) >= 31 and (y2)<= 56)):\n","    #     bBoxs1.append([x1, y1, x1+x2, y1+y2])\n","    # x3 = abs((x1-x2)//2)\n","    # x4 = (x1+x2)//2\n","    # y3 = abs((y1-y2)//2)\n","    # y4 = (y1+y2)//2\n","    # if(((x4-x3) >= 17 and (x4-x3) <= 44) and ((y4-y3) >= 31 and (y4-y3)<= 76) ):\n","    #     bBoxs2.append([x3, y3, x4, y4])\n","\n","print(\"No. Objects detected:\" ,len(bBoxs) )\n","print(\"No. Objects detected:\" ,len(machingbBoxes) )\n","# print(\"No. Objects detected:\" ,len(bBoxs1) )\n","# print(\"No. Objects detected:\" ,len(bBoxs2) )\n","# [x1, y1, x2, y2] = bBoxs[0]\n","# cv2.rectangle(image1, (x1, y1), (x2, y2), (0,255,0), 2)\n","\n","cv2.rectangle(image, (a1, b1), (a2, b2), (0,255,0), 2)\n","\n","for bBox in bBoxs[1:]:\n","    # print(bBox)\n","    [x1, y1, x2, y2] = bBox\n","    cv2.rectangle(image, (x1, y1), (x2, y2), (0,0,255), 2)\n","try:\n","    from google.colab.patches import cv2_imshow\n","    cv2_imshow(image)\n","except:\n","    print(\"using Local\")\n","    cv2.imshow(\"Input Image\", image)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R4mauefUMgYS"},"outputs":[],"source":["cv2.rectangle(image, (a1, b1), (a2, b2), (0,255,0), 2)\n","\n","for bBox in bBoxs[1:]:\n","    # print(bBox)\n","    [x1, y1, x2, y2] = bBox\n","    cv2.rectangle(image, (x1, y1), (x2, y2), (0,0,255), 2)\n","try:\n","    from google.colab.patches import cv2_imshow\n","    cv2_imshow(image)\n","except:\n","    print(\"using Local\")\n","    cv2.imshow(\"Input Image\", image)"]},{"cell_type":"code","source":["cv2.rectangle(image1, (a1, b1), (a2, b2), (0,255,0), 2)\n","\n","for bBox in machingbBoxes:\n","    [x1, y1, x2, y2] = bBox\n","    cv2.rectangle(image1, (x1, y1), (x2, y2), (0,0,255), 2)\n","try:\n","    from google.colab.patches import cv2_imshow\n","    cv2_imshow(image1)\n","except:\n","    print(\"using Local\")\n","    cv2.imshow(\"Input Image\", image1)"],"metadata":{"id":"OGCu3cfzRprW"},"execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python"},"orig_nbformat":4,"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"premium"},"nbformat":4,"nbformat_minor":0}