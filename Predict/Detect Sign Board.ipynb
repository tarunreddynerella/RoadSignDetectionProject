{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5.git\n",
    "!pip install -r yolov5/requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include all packages\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from yolov5.models.yolo import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "import torchvision\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile('/content/drive/MyDrive/DL Project/DataSet.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('./')\n",
    "except:\n",
    "    print(\"Using Local Machine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResizeImage(image: np.ndarray, x1: int, y1: int, x2: int, y2: int, newWidth: int, newHeight: int) -> tuple:\n",
    "    originalHeight, originalWidth = image.shape[:2]\n",
    "    scale = min(newWidth / originalWidth, newHeight / originalHeight)\n",
    "    resizedImage = cv2.resize(image, (round(originalWidth * scale), round(originalHeight * scale)), interpolation=cv2.INTER_LINEAR)\n",
    "    dx = round((newWidth - resizedImage.shape[1]) / 2)\n",
    "    dy = round((newHeight - resizedImage.shape[0]) / 2)\n",
    "    paddedImage = cv2.copyMakeBorder(resizedImage, dy, dy, dx, dx, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "    x1New, y1New = int(x1 * scale + dx), int(y1 * scale + dy)\n",
    "    x2New, y2New = int(x2 * scale + dx), int(y2 * scale + dy)\n",
    "    return paddedImage, x1New, y1New, x2New, y2New\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadDataSet(dataSetFolderPath: str) -> tuple:\n",
    "    images = []\n",
    "    annotations = []\n",
    "    annotationsFilePath = dataSetFolderPath+\"/allAnnotations.csv\"\n",
    "    annotationsDataFrame = pd.read_csv(annotationsFilePath, sep=\";\")\n",
    "    uniqueSigns = annotationsDataFrame['Annotation tag'].unique().tolist()\n",
    "    for index, row in annotationsDataFrame[1:].iterrows():\n",
    "        image = cv2.imread(dataSetFolderPath+\"/\"+row[0])\n",
    "        images.append(image)\n",
    "        annotations.append(\n",
    "            [row[2], row[3], row[4], row[5]])\n",
    "\n",
    "    del annotationsDataFrame\n",
    "\n",
    "    return images, annotations, len(uniqueSigns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PreProcessDataSet(images: list, annotations: list, batchSize: int, resize: tuple) -> tuple:\n",
    "    resizedImages = []\n",
    "    newAnnotations = []\n",
    "    for i, image in enumerate(images):\n",
    "        [x1, y1, x2, y2] = annotations[i]\n",
    "        resizedImage, x1New, y1New, x2New, y2New = ResizeImage(\n",
    "            image, x1, y1, x2, y2, resize[0], resize[1])\n",
    "        resizedImages.append(resizedImage)\n",
    "        newAnnotations.append(\n",
    "            [x1New, y1New, x2New, y2New])\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        resizedImages, newAnnotations, test_size=0.3, random_state=42)\n",
    "\n",
    "    return X_train, X_val, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputData, label = self.data[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            inputData = self.transform(inputData)\n",
    "        inputData = torch.from_numpy(inputData).float()\n",
    "        label = torch.tensor(label).float()\n",
    "        return inputData, label\n",
    "\n",
    "def CreateDataLoaders(X_train, X_val, y_train, y_val, batchSize):\n",
    "    trainDataSet = []\n",
    "    valDataSet = []\n",
    "    for i in range(len(X_train)):\n",
    "        trainDataSet.append((X_train[i], y_train[i]))\n",
    "\n",
    "    for i in range(len(X_val)):\n",
    "        valDataSet.append((X_val[i], y_val[i]))\n",
    "\n",
    "    trainDataSet = CustomDataset(trainDataSet)\n",
    "    valDataSet = CustomDataset(valDataSet)\n",
    "    trainDataLoader = DataLoader(\n",
    "        trainDataSet, batch_size=batchSize, shuffle=True, num_workers=4)\n",
    "    valDataLoader = DataLoader(\n",
    "        valDataSet, batch_size=batchSize, shuffle=False, num_workers=4)\n",
    "\n",
    "    return trainDataLoader, valDataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateYolov5Model(numClasses: int, version: str):\n",
    "    congfigFile = \"yolov5/models/yolov5{}.yaml\".format(version)\n",
    "    model = Model(congfigFile, ch=3, nc=numClasses)\n",
    "    # model.load_state_dict(torch.load(\"yolov5{}.pt\".format(version))[\"model\"].state_dict(), strict=False)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DetectImage(model, inputs, device, conf_thres=0.2, iou_thres=0.5):\n",
    "    model.eval()\n",
    "\n",
    "    inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "    inputs = inputs.unsqueeze(0)\n",
    "    inputs = inputs.permute(0, 3, 1, 2)\n",
    "    inputs = inputs.to(device)\n",
    "    conf_thres = torch.tensor(conf_thres)\n",
    "    with torch.no_grad():\n",
    "        output = model(inputs)\n",
    "        # max_conf_obj_idx = torch.argmax(output[0][..., 4:5], dim=1)\n",
    "        # output = output[0][torch.arange(output[0].size(0)), max_conf_obj_idx]\n",
    "        # output = torchvision.ops.nms(output, conf_thres, iou_thres)\n",
    "        # max_conf_obj_idx = torch.argmax(output[0][..., 4:5], dim=1)\n",
    "        # output = output[0][torch.arange(output[0].size(0)), max_conf_obj_idx]\n",
    "        output = output[0]\n",
    "        box_coordinates = output[..., :4].view(-1, 4)\n",
    "        confidence_scores = output[..., 4].view(-1)\n",
    "        nms_indices = torchvision.ops.nms(box_coordinates, confidence_scores, iou_thres)\n",
    "        output = output.view(-1, output.shape[-1])[nms_indices]\n",
    "    # Remove the batch dimension\n",
    "    output = output.squeeze(0)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 32\n",
    "inputShape = (640, 640)\n",
    "epochs = 100\n",
    "numAnchors = 3\n",
    "yolo5Version = 'm'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, annotations, numClasses = LoadDataSet(\"./DataSet\")\n",
    "numClasses = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = PreProcessDataSet(\n",
    "    images, annotations, batchSize, inputShape)\n",
    "del images\n",
    "del annotations\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainDataLoader, valDataLoader = CreateDataLoaders(\n",
    "#     X_train, X_val, y_train, y_val, batchSize)\n",
    "# del X_train\n",
    "# del y_train\n",
    "# del X_val\n",
    "# del y_val\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolov5Model = CreateYolov5Model(numClasses,yolo5Version)\n",
    "optimizer = optim.Adam(yolov5Model.parameters(), lr=0.001)\n",
    "yolov5Model = yolov5Model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savedModels = [ modelName for modelName in os.listdir('/content/drive/MyDrive/DL Project/Trained Models/') if ('yolov5Model' in modelName)]\n",
    "savedModels.sort()\n",
    "yolov5Model.load_state_dict(torch.load('/content/drive/MyDrive/DL Project/Trained Models/'+savedModels[-1], map_location=torch.device(device)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "randInt = random.randint(0,len(X_val))\n",
    "image = X_val[randInt]\n",
    "try:\n",
    "    from google.colab.patches import cv2_imshow\n",
    "    cv2_imshow(image)\n",
    "except:\n",
    "    print(\"using Local\")\n",
    "    cv2.imshow(\"Input Image\", image)\n",
    "\n",
    "predictions = DetectImage(yolov5Model, image, device)\n",
    "[a1,b1,a2,b2] = y_val[randInt]\n",
    "bBoxs = [[a1,b1,a2,b2]]\n",
    "i=0\n",
    "for pred in predictions:\n",
    "    i+=1\n",
    "    x1, y1, x2, y2, m1,m2 = pred\n",
    "    m1,m2, x1, y1, x2, y2= int(m1), int(m2),int(x1), int(y1), int(x2), int(y2)\n",
    "    if(a1 == x1 or a2 == x2 or b1 == y1 or b2 == y2 ):\n",
    "      bBoxs.append([x1, y1, x1+x2,y1+ y2])\n",
    "      \n",
    "print(\"No. Objects detected:\" ,len(bBoxs) )\n",
    "crtbBoxs = []\n",
    "for bBox in bBoxs:\n",
    "    for x in bBox:\n",
    "        if(x>=inputShape[0]):\n",
    "            crtbBoxs.append(bBox)\n",
    "\n",
    "print(\"No. Crt Objects detected:\" ,len(crtbBoxs) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x1, y1, x2, y2] = bBoxs[0]\n",
    "cv2.rectangle(image, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "for bBox in bBoxs[1:]:\n",
    "    crtbBox = True\n",
    "    for x in bBox:\n",
    "        if(x>=inputShape[0]):\n",
    "            crtbBox = False\n",
    "            break\n",
    "\n",
    "    if(crtbBox):\n",
    "        [x1, y1, x2, y2] = bBox\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0,0,255), 2)\n",
    "try:\n",
    "    from google.colab.patches import cv2_imshow\n",
    "    cv2_imshow(image)\n",
    "except:\n",
    "    print(\"using Local\")\n",
    "    cv2.imshow(\"Input Image\", image)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
