{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39900,"status":"ok","timestamp":1682789221506,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"NLuliN1UcKJ7","outputId":"59b3f7a4-1a3f-44bd-b045-ebce5644a0ad"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","Cloning into 'yolov5'...\n","remote: Enumerating objects: 15606, done.\u001b[K\n","remote: Counting objects: 100% (213/213), done.\u001b[K\n","remote: Compressing objects: 100% (155/155), done.\u001b[K\n","remote: Total 15606 (delta 101), reused 119 (delta 58), pack-reused 15393\u001b[K\n","Receiving objects: 100% (15606/15606), 14.64 MiB | 14.56 MiB/s, done.\n","Resolving deltas: 100% (10625/10625), done.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gitpython>=3.1.30\n","  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m733.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 6)) (3.7.1)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 7)) (1.22.4)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 8)) (4.7.0.72)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 9)) (8.4.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 10)) (5.9.5)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 11)) (6.0)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 12)) (2.27.1)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 13)) (1.10.1)\n","Collecting thop>=0.1.1\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 15)) (2.0.0+cu118)\n","Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 16)) (0.15.1+cu118)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 17)) (4.65.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 26)) (1.5.3)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 27)) (0.12.2)\n","Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.10/dist-packages (from -r yolov5/requirements.txt (line 41)) (67.7.2)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (2.8.2)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (4.39.3)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (23.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (3.0.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (0.11.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (1.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r yolov5/requirements.txt (line 12)) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r yolov5/requirements.txt (line 12)) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r yolov5/requirements.txt (line 12)) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->-r yolov5/requirements.txt (line 12)) (3.4)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r yolov5/requirements.txt (line 15)) (2.0.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r yolov5/requirements.txt (line 15)) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r yolov5/requirements.txt (line 15)) (3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r yolov5/requirements.txt (line 15)) (3.12.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r yolov5/requirements.txt (line 15)) (3.1.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7.0->-r yolov5/requirements.txt (line 15)) (4.5.0)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.0->-r yolov5/requirements.txt (line 15)) (16.0.2)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7.0->-r yolov5/requirements.txt (line 15)) (3.25.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r yolov5/requirements.txt (line 26)) (2022.7.1)\n","Collecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r yolov5/requirements.txt (line 6)) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7.0->-r yolov5/requirements.txt (line 15)) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7.0->-r yolov5/requirements.txt (line 15)) (1.3.0)\n","Installing collected packages: smmap, gitdb, gitpython, thop\n","Successfully installed gitdb-4.0.10 gitpython-3.1.31 smmap-5.0.0 thop-0.1.1.post2209072238\n"]}],"source":["try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    import zipfile\n","    with zipfile.ZipFile('/content/drive/MyDrive/DL Project/DataSet1.zip', 'r') as zip_ref:\n","        zip_ref.extractall('./DataSet1')\n","except:\n","    print(\"Using Local Machine\")\n","!git clone https://github.com/ultralytics/yolov5.git\n","!pip install -r yolov5/requirements.txt\n"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":7188,"status":"ok","timestamp":1682789228691,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"nQdle6ECbu-5"},"outputs":[],"source":["# Include all packages\n","import gc\n","import cv2\n","import shutil\n","import random\n","import numpy as np\n","import pandas as pd\n","from time import time\n","from copy import deepcopy\n","\n","from datetime import datetime\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","from yolov5.models.yolo import Model\n","from sklearn.model_selection import train_test_split\n","\n","import torchvision\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682789228691,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"SxOLsWl2cKJ8"},"outputs":[],"source":["def CannyEdge(capturedImage):\n","    grayScale = cv2.cvtColor(capturedImage, cv2.COLOR_BGR2GRAY)\n","    constrastKernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n","    topHat = cv2.morphologyEx(grayScale, cv2.MORPH_TOPHAT, constrastKernel)\n","    blackHat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, constrastKernel)\n","    grayScale = grayScale + topHat - blackHat\n","    gaussianImage = cv2.GaussianBlur(grayScale, (3, 3), 0)\n","    imageMedian = np.median(capturedImage)\n","    lowerThreshold = max(0, (0.7 * imageMedian))\n","    upperThreshold = min(255, (0.7 * imageMedian))\n","    cannyEdgeImage = cv2.Canny(gaussianImage, lowerThreshold, upperThreshold)\n","    cannyEdgeImage = cv2.bitwise_not(cannyEdgeImage)\n","    cannyEdgeImage = cv2.cvtColor(cannyEdgeImage, cv2.COLOR_GRAY2BGR)\n","    return cannyEdgeImage\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682789228691,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"Xd2tU8GB3nB3"},"outputs":[],"source":["def ResizeImage(image: np.ndarray, x1: int, y1: int, x2: int, y2: int, newWidth: int, newHeight: int) -> tuple:\n","    originalHeight, originalWidth = image.shape[:2]\n","    resizedImage = cv2.resize(\n","        image, (newWidth, newHeight), interpolation=cv2.INTER_LINEAR)\n","    widthScale = newWidth / originalWidth\n","    heightScale = newHeight / originalHeight\n","    x1New, y1New = int(x1 * widthScale), int(y1 * heightScale)\n","    x2New, y2New = int(x2 * widthScale), int(y2 * heightScale)\n","    return resizedImage, x1New, y1New, x2New, y2New\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1682789228691,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"9iMxCddibu-7"},"outputs":[],"source":["def LoadDataSet(dataSetFolderPath: str) -> tuple:\n","    images = []\n","    annotations = []\n","    resize = (640, 640)\n","    annotationsFilePath = dataSetFolderPath+\"/annotations.csv\"\n","    annotationsDataFrame = pd.read_csv(annotationsFilePath, sep=\",\")\n","    uniqueSigns = annotationsDataFrame['class'].unique().tolist()\n","    for index, row in annotationsDataFrame[1:].iterrows():\n","        image = cv2.imread(dataSetFolderPath+\"/\"+row[0])\n","        [classIndex, x1, y1, x2, y2] = [uniqueSigns.index(row[5]), row[1], row[2], row[3], row[4]]\n","        resizedImage, x1New, y1New, x2New, y2New = ResizeImage(image, x1, y1, x2, y2, resize[0], resize[1])\n","        # resizedImage = CannyEdge(resizedImage)\n","        images.append(resizedImage)\n","        annotations.append(\n","            [classIndex, x1New, y1New, x2New, y2New])\n","    del annotationsDataFrame\n","\n","    X_train, X_val, y_train, y_val = train_test_split(images, annotations, test_size=0.2, random_state=42)\n","\n","    return len(uniqueSigns), X_train, X_val, y_train, y_val\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1682789228691,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"68sCzcEAbu-7"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, data, transform=None):\n","        self.data = data\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        inputData, label = self.data[idx]\n","\n","        if self.transform:\n","            inputData = self.transform(inputData)\n","        inputData = torch.from_numpy(inputData).float()\n","        label = torch.tensor(label).float()\n","        return inputData, label\n","\n","\n","def CreateDataLoaders(X_train, X_val, y_train, y_val, batchSize):\n","    trainDataSet = []\n","    valDataSet = []\n","    for i in range(len(X_train)):\n","        trainDataSet.append((X_train[i], y_train[i]))\n","\n","    for i in range(len(X_val)):\n","        valDataSet.append((X_val[i], y_val[i]))\n","\n","    trainDataSet = CustomDataset(trainDataSet)\n","    valDataSet = CustomDataset(valDataSet)\n","    trainDataLoader = DataLoader(\n","        trainDataSet, batch_size=batchSize, shuffle=True, num_workers=4)\n","    valDataLoader = DataLoader(\n","        valDataSet, batch_size=batchSize, shuffle=False, num_workers=4)\n","\n","    return trainDataLoader, valDataLoader\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682789228692,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"owhzzQF61a0L"},"outputs":[],"source":["\n","def TargetstoTensors(targets, batchSize, numAnchors, gridSizes, numClasses):\n","    targetObj = []\n","    targetBox = []\n","    targetClass = []\n","    for grid_size in gridSizes:\n","        targetObj.append(torch.zeros((batchSize, numAnchors, grid_size, grid_size, 1)))\n","        targetBox.append(torch.zeros((batchSize, numAnchors, grid_size, grid_size, 4)))\n","        targetClass.append(torch.zeros((batchSize, numAnchors, grid_size, grid_size, numClasses)))\n","\n","    for batch_index, target in enumerate(targets):\n","        classindex, x1, y1, x2, y2 = target\n","        x_center, y_center, width, height = (x1 + x2) / 2, (y1 + y2) / 2, x2 - x1, y2 - y1\n","\n","        for i, grid_size in enumerate(gridSizes):\n","            x_cell, y_cell = int(x_center * grid_size), int(y_center * grid_size)\n","            anchor = 0\n","            try:\n","                targetObj[i][batch_index, anchor, y_cell, x_cell, 0] = 1\n","                targetBox[i][batch_index, anchor, y_cell, x_cell] = torch.tensor([x_center, y_center, width, height])\n","                targetClass[i][batch_index, anchor, y_cell, x_cell, classindex] = 1\n","            except Exception as e:\n","                pass\n","    return targetObj, targetBox, targetClass\n","\n","\n","class SignboardLoss(nn.Module):\n","    def __init__(self, num_anchors=3, num_classes=0):\n","        super(SignboardLoss, self).__init__()\n","        self.num_anchors = num_anchors\n","        self.num_classes = num_classes\n","\n","    def forward(self, preds, targets):\n","        objectLoss = torch.tensor(0.0, device=preds[0].device)\n","        boxLoss = torch.tensor(0.0, device=preds[0].device)\n","        classLoss = torch.tensor(0.0, device=preds[0].device)\n","        batchSize = preds[0].size(0)\n","        gridSizes = [pred.size(2) for pred in preds]\n","        targetObjList, targetBoxList, targetClassList = TargetstoTensors(targets, batchSize, self.num_anchors, gridSizes, self.num_classes)\n","\n","        for i, pred in enumerate(preds):\n","            targetObj = targetObjList[i].to(pred.device)\n","            targetBox = targetBoxList[i].to(pred.device)\n","            targetClass = targetClassList[i].to(pred.device)\n","\n","            objectLoss += nn.BCEWithLogitsLoss()(pred[..., 4:5], targetObj)\n","            boxLoss += nn.MSELoss()(pred[..., :4], targetBox)\n","            classLoss += nn.BCEWithLogitsLoss()(pred[..., 5:], targetClass)\n","\n","        total_loss = objectLoss + boxLoss + classLoss\n","        return total_loss\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682789228692,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"AZJ8-pMgbu-9"},"outputs":[],"source":["def CreateYolov5Model(numClasses: int, version: str, device):\n","    congfigFile = \"yolov5/models/yolov5{}.yaml\".format(version)\n","    model = Model(congfigFile, ch=3, nc=numClasses)\n","    ckpt = torch.load(f'yolov5{version}.pt', map_location=device)\n","    ckpt_model_dict = ckpt['model'].state_dict()\n","    compatible_weights = {k: v for k, v in ckpt_model_dict.items(\n","    ) if k in model.state_dict() and model.state_dict()[k].shape == v.shape}\n","    model.load_state_dict(compatible_weights, strict=False)\n","    model.hyp = ckpt['model'].hyp\n","    return model\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682789228692,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"bWVv6VcgcKJ-"},"outputs":[],"source":["def TrainEpoch(model, dataLoader, optimizer, lossFunction, device):\n","    print(\"Training Epoch\")\n","    model.train()\n","    runningLoss = 0\n","    dataLoaderLen = len(dataLoader)\n","    for i, (inputs, targets) in enumerate(dataLoader):\n","        # inputs = inputs.permute(2, 0, 1)\n","        inputs = inputs.permute(0, 3, 1, 2)\n","        inputs = inputs.to(device)\n","        targets = targets.to(device)\n","        optimizer.zero_grad()\n","        with torch.set_grad_enabled(True):\n","            outputs = model(inputs)\n","            loss = lossFunction(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","\n","        runningLoss += loss.item() * inputs.size(0)\n","        if(((i*100)//dataLoaderLen) % 10 == 0):\n","            print((i*100//dataLoaderLen), end=\"%,\")\n","    print()\n","    epochLoss = runningLoss / dataLoaderLen\n","    return model, epochLoss\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682789228692,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"g3l_D4syBmlL"},"outputs":[],"source":["def ValidateEpoch(model, dataLoader, lossFunction, device):\n","    print(\"Validating Epoch\")\n","    model.eval()\n","    runningLoss = 0\n","    dataLoaderLen = len(dataLoader)\n","    for i, (inputs, targets) in enumerate(dataLoader):\n","        inputs = inputs.permute(0, 3, 1, 2)\n","        inputs = inputs.to(device)\n","        targets = targets.to(device)\n","        with torch.set_grad_enabled(False):\n","            outputs = model(inputs)\n","            loss = lossFunction(outputs, targets)\n","        runningLoss += loss.item() * inputs.size(0)\n","        if(((i*100)//dataLoaderLen) % 10 == 0):\n","            print((i*100//dataLoaderLen), end=\"%,\")\n","    print()\n","    epochLoss = runningLoss / dataLoaderLen\n","    return epochLoss\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1682789228692,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"GCtFNZeAcKJ-"},"outputs":[],"source":["def DetectImage(model, inputs, device, conf_thres=0.2, iou_thres=0.5):\n","    model.eval()\n","\n","    inputs = torch.tensor(inputs, dtype=torch.float32)\n","    inputs = inputs.unsqueeze(0)\n","    inputs = inputs.permute(0, 3, 1, 2)\n","    inputs = inputs.to(device)\n","    conf_thres = torch.tensor(conf_thres)\n","    with torch.no_grad():\n","        output = model(inputs)\n","        output = output[0]\n","        confidences = output[..., 4:5]\n","        max_confidences, max_indices = torch.max(confidences, dim=1)\n","        box_coordinates = output[..., :4].view(-1, 4)\n","        confidence_scores = output[..., 4].view(-1)\n","        nms_indices = torchvision.ops.nms(box_coordinates, confidence_scores, iou_thres)\n","        # output = output.view(-1, output.shape[-1])[nms_indices]\n","        output = output.view(-1, output.shape[-1])[max_indices]\n","    output = output.squeeze(0)\n","    return output\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1682789228692,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"Vmu96jtn_Xv5"},"outputs":[],"source":["\n","def EvaluateModel(yolov5Model, X_val: list, y_val: list, device ):\n","    randInt = random.randint(0,len(X_val))\n","    image = X_val[randInt]\n","    image1 = deepcopy(image)\n","    predictions = DetectImage(yolov5Model, image, device)\n","    [a1,b1,a2,b2] = y_val[randInt]\n","    bBoxs = []\n","    machingbBoxes = []\n","    albBoxs =[]\n","\n","    for pred in predictions:\n","        x1, y1, x2, y2, m1,m2 = pred[:6]\n","        m1,m2, x1, y1, x2, y2= int(m1), int(m2),int(x1), int(y1), int(x2), int(y2)\n","        if(a1 == x1 or a2 == x2 or b1 == y1 or b2 == y2 ):\n","            if(((x1-x2) >= 17 and (x1-x2) <= 32) and ((y1-y2) >= 31 and (y1-y2)<= 56) ):\n","                machingbBoxes.append([x1, y1, x2,y2])\n","        \n","        if(abs(x1-x2) >= 17  and abs(y1-y2) >= 31 ):\n","            albBoxs.append([x1, y1, x2, y2])\n","\n","        if((abs(x1-x2) >= 17 and abs(x1-x2) <= 32) and (abs(y1-y2) >= 31 and abs(y1-y2)<= 56) ):\n","            bBoxs.append([x1, y1, x2, y2])\n","        \n","        \n","        # x_center, y_center, width, height =x1, y1, x2, y2\n","        # x1 = x_center - (width // 2)\n","        # y1 = y_center - (height // 2)\n","        # x2 = x_center + (width // 2)\n","        # y2 = y_center + (height // 2)\n","\n","    print(\"No. machingbBoxes detected:\" ,len(machingbBoxes) )\n","    print(\"No. albBoxs detected:\" ,len(albBoxs) )\n","    print(\"No. bBoxs detected:\" ,len(bBoxs) )\n","\n","\n","\n","\n","    for bBox in machingbBoxes:\n","        [x1, y1, x2, y2] = bBox\n","        cv2.rectangle(image, (x1, y1), (x2, y2), (0,0,0), 2)\n","\n","    for bBox in albBoxs:\n","        [x1, y1, x2, y2] = bBox\n","        cv2.rectangle(image, (x1, y1), (x2, y2), (0,0,255), 2)\n","\n","    for bBox in bBoxs:\n","        [x1, y1, x2, y2] = bBox\n","        cv2.rectangle(image, (x1, y1), (x2, y2), (255,0,0), 2)\n","\n","    cv2.rectangle(image, (a1, b1), (a2, b2), (0,255,0), 2)\n","\n","\n","    try:\n","        from google.colab.patches import cv2_imshow\n","        cv2_imshow(image)\n","    except:\n","        print(\"using Local\")\n","        cv2.imshow(\"Input Image\", image)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682789228693,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"YbVy1m6T3nB6"},"outputs":[],"source":["def TrainModel(model, trainDataLoader, valDataLoader, epochs, optimizer, scheduler, lossFunction, device):\n","    for epoch in range(epochs):\n","        startTime = time()\n","        print(\"Epoch {}/{}:\".format(epoch+1, epochs))\n","        startTime = time()\n","        model, trainingEpochLoss = TrainEpoch(model, trainDataLoader, optimizer, lossFunction, device)\n","        validationEpochLoss = ValidateEpoch(model, valDataLoader, lossFunction, device)\n","        scheduler.step(validationEpochLoss)\n","        scheduler.step(trainingEpochLoss)\n","        endTime = time()\n","        timeTaken = endTime - startTime\n","        print()\n","        print(\"Training Loss: {:.4f}\".format(trainingEpochLoss))\n","        print(\"validation Loss: {:.4f}\".format(validationEpochLoss))\n","        print(\"Time taken: {}min, {}, secs\".format(timeTaken//60, int(timeTaken % 60)))\n","    \n","    print(\"Training complete.\")\n","    return model\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1682789229546,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"n4adeyZX3nB6"},"outputs":[],"source":["batchSize = 32\n","inputShape = (640, 640)\n","epochs = 100\n","numAnchors = 3\n","yolo5Version = 's'\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1682789229546,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"A_imN2T83nB6","outputId":"04adf1cd-c96c-447f-a899-72b212b134a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda device\n"]}],"source":["print(\"Using {} device\".format(device))\n"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1430,"status":"ok","timestamp":1682789230971,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"h5e6cK8HcKJ_","outputId":"3329e4df-4a17-44f1-98b8-fe0075260444"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading Weights of yolo5 Verion  s\n","--2023-04-29 17:27:13--  https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5s.pt\n","Resolving github.com (github.com)... 20.27.177.113\n","Connecting to github.com (github.com)|20.27.177.113|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/56dd3480-9af3-11eb-9c92-3ecd167961dc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230429%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230429T172713Z&X-Amz-Expires=300&X-Amz-Signature=4e0745075e067899773f17e2d442369540da7917a9ebf5b1fa2a06c15109192d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=264818686&response-content-disposition=attachment%3B%20filename%3Dyolov5s.pt&response-content-type=application%2Foctet-stream [following]\n","--2023-04-29 17:27:13--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/264818686/56dd3480-9af3-11eb-9c92-3ecd167961dc?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230429%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230429T172713Z&X-Amz-Expires=300&X-Amz-Signature=4e0745075e067899773f17e2d442369540da7917a9ebf5b1fa2a06c15109192d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=264818686&response-content-disposition=attachment%3B%20filename%3Dyolov5s.pt&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 14795158 (14M) [application/octet-stream]\n","Saving to: ‘yolov5s.pt’\n","\n","yolov5s.pt          100%[===================>]  14.11M  26.6MB/s    in 0.5s    \n","\n","2023-04-29 17:27:14 (26.6 MB/s) - ‘yolov5s.pt’ saved [14795158/14795158]\n","\n"]}],"source":["print(\"Downloading Weights of yolo5 Verion \", yolo5Version)\n","weightsURL = \"https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5{}.pt\".format(\n","    yolo5Version)\n","!wget {weightsURL}\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8452,"status":"ok","timestamp":1682789239422,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"Z884-9l6bu--","outputId":"328c3020-0f73-4bed-f834-d4fc058c079a"},"outputs":[{"name":"stdout","output_type":"stream","text":["9\n"]},{"data":{"text/plain":["4"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["numClasses, X_train, X_val, y_train, y_val = LoadDataSet(\"./DataSet1\")\n","print(numClasses)\n","gc.collect()\n"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1682789239422,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"F5Roj9e3BmlM"},"outputs":[],"source":["# try:\n","#     from google.colab.patches import cv2_imshow\n","#     cv2_imshow(X_train[10])\n","# except:\n","#     print(\"using Local\")\n","#     cv2.imshow(\"Input Image\", X_train[10])"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1682789239423,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"t0Vr59qSBmlN"},"outputs":[],"source":["trainDataLoader, valDataLoader = CreateDataLoaders(\n","    X_train, X_val, y_train, y_val, batchSize)\n"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6793,"status":"ok","timestamp":1682789246205,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"WJrRAHOrbu--","outputId":"f0c5a931-ea45-481e-de99-c04bd8c07aac"},"outputs":[{"name":"stderr","output_type":"stream","text":["Overriding model.yaml nc=80 with nc=9\n","\n","                 from  n    params  module                                  arguments                     \n","  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n","  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n","  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n","  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n","  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n","  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n","  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n","  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n","  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n","  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n"," 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n"," 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n"," 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n"," 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n"," 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n"," 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n"," 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n"," 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n"," 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n"," 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n"," 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n"," 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n"," 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n"," 24      [17, 20, 23]  1     37758  yolov5.models.yolo.Detect               [9, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n","YOLOv5s summary: 214 layers, 7043902 parameters, 7043902 gradients, 16.0 GFLOPs\n","\n"]}],"source":["yolov5Model = CreateYolov5Model(numClasses, yolo5Version, device)\n","optimizer = optim.Adam(yolov5Model.parameters(), lr=0.01)\n","yolov5LossFunction= SignboardLoss(num_classes = numClasses)\n","yolov5Model = yolov5Model.to(device)\n","yolov5LossFunction = yolov5LossFunction.to(device)\n","scheduler = ReduceLROnPlateau(\n","    optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":52200,"status":"error","timestamp":1682789545288,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"9Atpoji13nB7","outputId":"4213798b-b682-47c5-a7d9-a5e52f8ef32c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/300:\n","Training Epoch\n","0%,30%,60%,\n","Validating Epoch\n","0%,50%,\n","\n","Training Loss: 15.7187\n","validation Loss: 1.2611\n","Time taken: 0.0min, 20, secs\n","Epoch 2/300:\n","Training Epoch\n","0%,30%,60%,\n","Validating Epoch\n","0%,50%,\n","\n","Training Loss: 0.5714\n","validation Loss: 0.2671\n","Time taken: 0.0min, 13, secs\n","Epoch 3/300:\n","Training Epoch\n","0%,30%,60%,\n","Validating Epoch\n","0%,50%,\n","\n","Training Loss: 0.1388\n","validation Loss: 0.0665\n","Time taken: 0.0min, 13, secs\n","Epoch 4/300:\n","Training Epoch\n","0%,30%,60%,\n","Validating Epoch\n","0%,50%,\n","\n","Training Loss: 0.0602\n","validation Loss: 0.0366\n","Time taken: 0.0min, 13, secs\n","Epoch 5/300:\n","Training Epoch\n","0%,30%,60%,\n","Validating Epoch\n","0%,50%,\n","\n","Training Loss: 0.0347\n","validation Loss: 0.0384\n","Time taken: 0.0min, 13, secs\n","Epoch 6/300:\n","Training Epoch\n","0%,30%,60%,\n","Validating Epoch\n","0%,50%,\n","\n","Training Loss: 0.0338\n","validation Loss: 0.0321\n","Time taken: 0.0min, 13, secs\n","Epoch 7/300:\n","Training Epoch\n","0%,30%,60%,\n","Validating Epoch\n","0%,50%,\n","\n","Training Loss: 0.0308\n","validation Loss: 0.0415\n","Time taken: 0.0min, 13, secs\n","Epoch 8/300:\n","Training Epoch\n","0%,30%,60%,\n","Validating Epoch\n","0%,50%,\n","\n","Training Loss: 0.0262\n","validation Loss: 0.0219\n","Time taken: 0.0min, 13, secs\n","Epoch 9/300:\n","Training Epoch\n","0%,30%,60%,\n","Validating Epoch\n","0%,50%,\n","\n","Training Loss: 0.0199\n","validation Loss: 0.0158\n","Time taken: 0.0min, 13, secs\n","Epoch 10/300:\n","Training Epoch\n","0%,30%,60%,\n","Validating Epoch\n","0%,50%,\n","\n","Training Loss: 0.0187\n","validation Loss: 0.0219\n","Time taken: 0.0min, 13, secs\n","Epoch 11/300:\n","Training Epoch\n","0%,30%,60%,\n","Validating Epoch\n","0%,50%,\n","\n","Training Loss: 0.0156\n","validation Loss: 0.0141\n","Time taken: 0.0min, 13, secs\n","Epoch 12/300:\n","Training Epoch\n","0%,30%,60%,\n","Validating Epoch\n","0%,50%,\n","\n","Training Loss: 0.0138\n","validation Loss: 0.0118\n","Time taken: 0.0min, 13, secs\n","Epoch 13/300:\n","Training Epoch\n","0%,30%,60%,\n","Validating Epoch\n","0%,50%,\n","\n","Training Loss: 0.0139\n","validation Loss: 0.0146\n","Time taken: 0.0min, 13, secs\n","Epoch 14/300:\n","Training Epoch\n","0%,30%,60%,\n","Validating Epoch\n","0%,50%,\n","\n","Training Loss: 0.0120\n","validation Loss: 0.0118\n","Time taken: 0.0min, 13, secs\n","Epoch 15/300:\n","Training Epoch\n","0%,30%,60%,\n","Validating Epoch\n","0%,50%,\n","\n","Training Loss: 0.0106\n","validation Loss: 0.0115\n","Time taken: 0.0min, 13, secs\n","Epoch 16/300:\n","Training Epoch\n","0%,30%,60%,\n","Validating Epoch\n","0%,50%,\n","\n","Training Loss: 0.0125\n","validation Loss: 0.0156\n","Time taken: 0.0min, 13, secs\n","Epoch 17/300:\n","Training Epoch\n","0%,30%,60%,\n","Validating Epoch\n","0%,50%,\n","\n","Training Loss: 0.0123\n","validation Loss: 0.0158\n","Time taken: 0.0min, 13, secs\n","Epoch 18/300:\n","Training Epoch\n","0%,30%,60%,\n","Validating Epoch\n","0%,50%,\n","\n","Training Loss: 0.0138\n","validation Loss: 0.0108\n","Time taken: 0.0min, 13, secs\n","Epoch 19/300:\n","Training Epoch\n","0%,30%,60%,\n","Validating Epoch\n","0%,50%,\n","\n","Training Loss: 0.0122\n","validation Loss: 0.0218\n","Time taken: 0.0min, 13, secs\n","Epoch 20/300:\n","Training Epoch\n","0%,30%,60%,\n","Validating Epoch\n","0%,50%,\n","\n","Training Loss: 0.0121\n","validation Loss: 0.0128\n","Time taken: 0.0min, 13, secs\n","Epoch 21/300:\n","Training Epoch\n","0%,30%,60%,\n","Validating Epoch\n","0%,50%,\n","\n","Training Loss: 0.0096\n","validation Loss: 0.0098\n","Time taken: 0.0min, 13, secs\n","Epoch 22/300:\n","Training Epoch\n","0%,30%,60%,\n","Validating Epoch\n","0%,50%,\n","\n","Training Loss: 0.0089\n","validation Loss: 0.0179\n","Time taken: 0.0min, 13, secs\n","Epoch 23/300:\n","Training Epoch\n","0%,"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-bab59970196e>\u001b[0m in \u001b[0;36mTargetstoTensors\u001b[0;34m(targets, batchSize, numAnchors, gridSizes, numClasses)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mtargetObj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0mtargetBox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cell\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 10680 is out of bounds for dimension 2 with size 40","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-ed6ec53540dd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainedModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myolov5Model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDataLoader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myolov5LossFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-13-ca81e58eaf3f>\u001b[0m in \u001b[0;36mTrainModel\u001b[0;34m(model, trainDataLoader, valDataLoader, epochs, optimizer, scheduler, lossFunction, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {}/{}:\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mstartTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainingEpochLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainEpoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlossFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mvalidationEpochLoss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mValidateEpoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlossFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidationEpochLoss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-005b959c4ba8>\u001b[0m in \u001b[0;36mTrainEpoch\u001b[0;34m(model, dataLoader, optimizer, lossFunction, device)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-bab59970196e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, preds, targets)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mbatchSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mgridSizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mtargetObjList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetBoxList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetClassList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTargetstoTensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgridSizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-bab59970196e>\u001b[0m in \u001b[0;36mTargetstoTensors\u001b[0;34m(targets, batchSize, numAnchors, gridSizes, numClasses)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0manchor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mtargetObj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0mtargetBox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cell\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0mtargetClass\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["trainedModel = TrainModel(yolov5Model, trainDataLoader,valDataLoader, epochs, optimizer, scheduler, yolov5LossFunction, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"aborted","timestamp":1682789545289,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"rGDk5r7d3nB7"},"outputs":[],"source":["date = datetime.now()\n","date = date.strftime(\"%m-%d-%H\")\n","torch.save(trainedModel.state_dict(), 'yolov5Modelv4-' + date +'.pth')\n","shutil.copy('/content/yolov5Modelv4-' + date +'.pth', '/content/drive/MyDrive/DL Project/Trained Models/yolov5Modelv4-' + date +'.pth')\n"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
