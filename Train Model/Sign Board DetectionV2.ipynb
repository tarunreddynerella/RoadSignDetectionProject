{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Gi1JVb63nB1",
        "outputId": "4556ee59-cb00-491f-95c4-acb9962b7499"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5.git\n",
        "!pip install -r yolov5/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQdle6ECbu-5"
      },
      "outputs": [],
      "source": [
        "# Include all packages\n",
        "import gc\n",
        "import cv2\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from time import time\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from yolov5.models.yolo import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "\n",
        "import torchvision\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggf594ltbu-6"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile('/content/drive/MyDrive/DL Project/DataSet1.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('./DataSet1')\n",
        "except:\n",
        "    print(\"Using Local Machine\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def CannyEdge(capturedImage):\n",
        "#     grayScale = cv2.cvtColor(capturedImage, cv2.COLOR_BGR2GRAY)\n",
        "#     constrastKernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5) )\n",
        "#     topHat = cv2.morphologyEx(grayScale, cv2.MORPH_TOPHAT, constrastKernel)\n",
        "#     blackHat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, constrastKernel)\n",
        "#     grayScale = grayScale + topHat - blackHat\n",
        "#     bilateralFilter = cv2.bilateralFilter(grayScale, 11, 17, 17)\n",
        "#     imageMedian = np.median(capturedImage)\n",
        "#     lowerThreshold = max(0, (0.7 * imageMedian))\n",
        "#     upperThreshold = min(255, (0.7 * imageMedian))\n",
        "#     cannyEdgeImage = cv2.Canny(bilateralFilter, lowerThreshold, upperThreshold)\n",
        "#     cannyEdgeImage = cv2.bitwise_not(cannyEdgeImage)\n",
        "#     cannyEdgeImage = cv2.cvtColor(cannyEdgeImage, cv2.COLOR_GRAY2BGR)\n",
        "#     return cannyEdgeImage\n",
        "\n",
        "\n",
        "\n",
        "# GPT's SUGGESTIONS :\n",
        "\n",
        "def CannyEdge(capturedImage):\n",
        "    grayScale = cv2.cvtColor(capturedImage, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    # Optional: Apply a faster denoising method if needed.\n",
        "    # grayScale = cv2.GaussianBlur(grayScale, (3, 3), 0)\n",
        "    \n",
        "    imageMedian = np.median(capturedImage)\n",
        "    lowerThreshold = max(0, (0.7 * imageMedian))\n",
        "    upperThreshold = min(255, (0.7 * imageMedian))\n",
        "    cannyEdgeImage = cv2.Canny(grayScale, lowerThreshold, upperThreshold)\n",
        "    cannyEdgeImage = cv2.bitwise_not(cannyEdgeImage)\n",
        "    \n",
        "    return cannyEdgeImage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xd2tU8GB3nB3"
      },
      "source": [
        "def ResizeImage(image: np.ndarray, x1: int, y1: int, x2: int, y2: int, newWidth: int, newHeight: int) -> tuple:\n",
        "    originalHeight, originalWidth = image.shape[:2]\n",
        "    widthScale = newWidth / originalWidth\n",
        "    heightScale = newHeight / originalHeight\n",
        "    resizedImage = cv2.resize(\n",
        "        image, (newWidth, newHeight), interpolation=cv2.INTER_LINEAR)\n",
        "    x1New, y1New = int(x1 * widthScale), int(y1 * heightScale)\n",
        "    x2New, y2New = int(x2 * widthScale), int(y2 * heightScale)\n",
        "    return resizedImage, x1New, y1New, x2New, y2New\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iMxCddibu-7"
      },
      "source": [
        "def LoadDataSet(dataSetFolderPath: str) -> tuple:\n",
        "    images = []\n",
        "    annotations = []\n",
        "    annotationsFilePath = dataSetFolderPath+\"/annotations.csv\"\n",
        "    annotationsDataFrame = pd.read_csv(annotationsFilePath, sep=\",\")\n",
        "    uniqueSigns = annotationsDataFrame['class'].unique().tolist()\n",
        "    for index, row in annotationsDataFrame[1:].iterrows():\n",
        "        image = cv2.imread(dataSetFolderPath+\"/\"+row[0])\n",
        "        images.append(image)\n",
        "        annotations.append(\n",
        "            [row[1], row[2], row[3], row[4]])\n",
        "\n",
        "    del annotationsDataFrame\n",
        "\n",
        "    return images, annotations, len(uniqueSigns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87wkTQy53nB4"
      },
      "source": [
        "def PreProcessDataSet(images: list, annotations: list, batchSize: int, resize: tuple) -> tuple:\n",
        "    resizedImages = []\n",
        "    newAnnotations = []\n",
        "    for i, image in enumerate(images):\n",
        "        [x1, y1, x2, y2] = annotations[i]\n",
        "        resizedImage, x1New, y1New, x2New, y2New = ResizeImage(\n",
        "            image, x1, y1, x2, y2, resize[0], resize[1])\n",
        "        resizedImage = CannyEdge(resizedImage)\n",
        "        resizedImages.append(resizedImage)\n",
        "        newAnnotations.append(\n",
        "            [x1New, y1New, x2New, y2New])\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        resizedImages, newAnnotations, test_size=0.2, random_state=42)\n",
        "\n",
        "    return X_train, X_val, y_train, y_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPT's SUGGESTIONS:\n",
        "# When resizing the image, you can directly use the cv2.resize() function without calculating the width and height scale separately. This simplifies the code and makes it more readable.\n",
        "# The ResizeImage function can be modified to accept the bounding box as a tuple, which makes the function call cleaner.\n",
        "# In the LoadDataSet function, consider using a more efficient way of reading and appending images and annotations, such as list comprehensions.\n",
        "# In the PreProcessDataSet function, it might be better to apply the CannyEdge function after splitting the data into training and validation sets. This way, you only process the images that will be used for training or validation.\n",
        "\n",
        "\n",
        "def ResizeImage(image: np.ndarray, bbox: tuple, newWidth: int, newHeight: int) -> tuple:\n",
        "    originalHeight, originalWidth = image.shape[:2]\n",
        "    resizedImage = cv2.resize(image, (newWidth, newHeight), interpolation=cv2.INTER_LINEAR)\n",
        "    x1New, y1New = int(bbox[0] * newWidth / originalWidth), int(bbox[1] * newHeight / originalHeight)\n",
        "    x2New, y2New = int(bbox[2] * newWidth / originalWidth), int(bbox[3] * newHeight / originalHeight)\n",
        "    return resizedImage, (x1New, y1New, x2New, y2New)\n",
        "\n",
        "def LoadDataSet(dataSetFolderPath: str) -> tuple:\n",
        "    annotationsFilePath = dataSetFolderPath+\"/annotations.csv\"\n",
        "    annotationsDataFrame = pd.read_csv(annotationsFilePath, sep=\",\")\n",
        "    uniqueSigns = annotationsDataFrame['class'].unique().tolist()\n",
        "    \n",
        "    images = [cv2.imread(dataSetFolderPath+\"/\"+row[0]) for _, row in annotationsDataFrame[1:].iterrows()]\n",
        "    annotations = [list(map(int, row[1:5])) for _, row in annotationsDataFrame[1:].iterrows()]\n",
        "\n",
        "    del annotationsDataFrame\n",
        "\n",
        "    return images, annotations, len(uniqueSigns)\n",
        "\n",
        "def PreProcessDataSet(images: list, annotations: list, batchSize: int, resize: tuple) -> tuple:\n",
        "    resizedImages, newAnnotations = zip(*[ResizeImage(image, bbox, resize[0], resize[1]) for image, bbox in zip(images, annotations)])\n",
        "    \n",
        "    X_train, X_val, y_train, y_val = train_test_split(resizedImages, newAnnotations, test_size=0.2, random_state=42)\n",
        "\n",
        "    X_train = [CannyEdge(image) for image in X_train]\n",
        "    X_val = [CannyEdge(image) for image in X_val]\n",
        "\n",
        "    return X_train, X_val, y_train, y_val\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68sCzcEAbu-7"
      },
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        inputData, label = self.data[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            inputData = self.transform(inputData)\n",
        "        inputData = torch.from_numpy(inputData).float()\n",
        "        label = torch.tensor(label).float()\n",
        "        return inputData, label\n",
        "\n",
        "def CreateDataLoaders(X_train, X_val, y_train, y_val, batchSize):\n",
        "    trainDataSet = []\n",
        "    valDataSet = []\n",
        "    for i in range(len(X_train)):\n",
        "        trainDataSet.append((X_train[i], y_train[i]))\n",
        "\n",
        "    for i in range(len(X_val)):\n",
        "        valDataSet.append((X_val[i], y_val[i]))\n",
        "\n",
        "    trainDataSet = CustomDataset(trainDataSet)\n",
        "    valDataSet = CustomDataset(valDataSet)\n",
        "    trainDataLoader = DataLoader(\n",
        "        trainDataSet, batch_size=batchSize, shuffle=True, num_workers=4)\n",
        "    valDataLoader = DataLoader(\n",
        "        valDataSet, batch_size=batchSize, shuffle=False, num_workers=4)\n",
        "\n",
        "    return trainDataLoader, valDataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPT's SUGGESTIONS :\n",
        "# Simplified the creation of trainDataSet and valDataSet by using the zip() function and converting the zipped object to a list. This makes the code more concise and easier to read.\n",
        "# Removed the unnecessary for loop when creating the train and validation datasets.\n",
        "# Replaced torch.from_numpy() with torch.tensor() to directly create a tensor from the input data. This makes the code more consistent.\n",
        "# Added the pin_memory=True argument to both the trainDataLoader and valDataLoader. This allows faster data transfer between the CPU and GPU, which can improve training performance when using a GPU.\n",
        "\n",
        "# Custom Dataset class that inherits from torch.utils.data.Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    # Initialize the CustomDataset class with data and transform\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "\n",
        "    # Define the length method to return the length of the dataset\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    # Define the getitem method to return input data and labels as tensors\n",
        "    def __getitem__(self, idx):\n",
        "        inputData, label = self.data[idx]\n",
        "\n",
        "        # Apply transform if provided\n",
        "        if self.transform:\n",
        "            inputData = self.transform(inputData)\n",
        "\n",
        "        # Convert inputData to a PyTorch tensor\n",
        "        inputData = torch.tensor(inputData, dtype=torch.float32)\n",
        "\n",
        "        # Convert label to a PyTorch tensor\n",
        "        label = torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "        return inputData, label\n",
        "\n",
        "# Function to create DataLoaders for train and validation datasets\n",
        "def CreateDataLoaders(X_train, X_val, y_train, y_val, batchSize):\n",
        "    # Combine input data and labels into a single list of tuples for train and validation datasets\n",
        "    trainDataSet = list(zip(X_train, y_train))\n",
        "    valDataSet = list(zip(X_val, y_val))\n",
        "\n",
        "    # Create CustomDataset objects for train and validation datasets\n",
        "    trainDataSet = CustomDataset(trainDataSet)\n",
        "    valDataSet = CustomDataset(valDataSet)\n",
        "\n",
        "    # Create DataLoaders for train and validation datasets\n",
        "    trainDataLoader = DataLoader(trainDataSet, batch_size=batchSize, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    valDataLoader = DataLoader(valDataSet, batch_size=batchSize, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    return trainDataLoader, valDataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owhzzQF61a0L"
      },
      "source": [
        "\n",
        "def TargetstoTensors(targets, batchSize, numAnchors, gridSizes):\n",
        "    targetObj = []\n",
        "    targetBox = []\n",
        "    for grid_size in gridSizes:\n",
        "        targetObj.append(torch.zeros((batchSize, numAnchors, grid_size, grid_size, 1)))\n",
        "        targetBox.append(torch.zeros((batchSize, numAnchors, grid_size, grid_size, 4)))\n",
        "\n",
        "    for batch_index, target in enumerate(targets):\n",
        "        x1, y1, x2, y2 = target.long()\n",
        "        x_center, y_center, width, height = (x1 + x2) / 2, (y1 + y2) / 2, x2 - x1, y2 - y1\n",
        "\n",
        "        for i, grid_size in enumerate(gridSizes):\n",
        "            x_cell, y_cell = int(x_center * grid_size), int(y_center * grid_size)\n",
        "            anchor = 0\n",
        "            try:\n",
        "                targetObj[i][batch_index, anchor, y_cell, x_cell, 0] = 1\n",
        "                targetBox[i][batch_index, anchor, y_cell, x_cell] = torch.tensor([x_center, y_center, width, height])\n",
        "            except Exception as e:\n",
        "                pass\n",
        "    return targetObj, targetBox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCXFHrH5zfQx"
      },
      "source": [
        "\n",
        "class SignboardLoss(nn.Module):\n",
        "    def __init__(self, num_anchors=3):\n",
        "        super(SignboardLoss, self).__init__()\n",
        "        self.num_anchors = num_anchors\n",
        "\n",
        "    def forward(self, preds, targets):\n",
        "        objectLoss = torch.tensor(0.0, device=preds[0].device)\n",
        "        boxLoss = torch.tensor(0.0, device=preds[0].device)\n",
        "        batchSize = preds[0].size(0)\n",
        "        gridSizes = [pred.size(2) for pred in preds]\n",
        "        targetObjList, targetBoxList = TargetstoTensors(targets, batchSize, self.num_anchors, gridSizes)\n",
        "\n",
        "        for i, pred in enumerate(preds):\n",
        "            targetObj = targetObjList[i].to(pred.device)\n",
        "            targetBox = targetBoxList[i].to(pred.device)\n",
        "\n",
        "            objectLoss += nn.BCEWithLogitsLoss()(pred[..., 4:5], targetObj)\n",
        "            boxLoss += nn.MSELoss()(pred[..., :4], targetBox)\n",
        "\n",
        "        total_loss = objectLoss + boxLoss\n",
        "        return total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZJ8-pMgbu-9"
      },
      "source": [
        "def CreateYolov5Model(numClasses: int, version: str):\n",
        "    congfigFile = \"yolov5/models/yolov5{}.yaml\".format(version)\n",
        "    model = Model(congfigFile, ch=3, nc=numClasses)\n",
        "    # model.load_state_dict(torch.load(\"yolov5{}.pt\".format(version))[\"model\"].state_dict(), strict=False)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPT's suggestions:\n",
        "# Use a more suitable anchor selection method to determine the best anchor for each ground truth box. One approach could be to calculate the Intersection over Union (IoU) between the ground truth box and the anchors and assign the anchor with the highest IoU.\n",
        "# Use a better loss function for the bounding box regression. The Mean Squared Error loss might not be the best option. Instead, you can use the Smooth L1 Loss or the IoU loss, which are more robust to outliers.\n",
        "# Normalize the target bounding box coordinates to be in the range [0, 1]. This helps in training the model.\n",
        "# You'll need to define the anchors variable and implement the calculate_iou function. The anchors variable should store the pre-defined anchor boxes for each grid size. The calculate_iou function should compute the Intersection over Union (IoU) between two bounding boxes\n",
        "\n",
        "\n",
        "def TargetstoTensors(targets, batchSize, numAnchors, gridSizes, anchors):\n",
        "    targetObj = []\n",
        "    targetBox = []\n",
        "    for grid_size in gridSizes:\n",
        "        targetObj.append(torch.zeros((batchSize, numAnchors, grid_size, grid_size, 1)))\n",
        "        targetBox.append(torch.zeros((batchSize, numAnchors, grid_size, grid_size, 4)))\n",
        "\n",
        "    for batch_index, target in enumerate(targets):\n",
        "        x1, y1, x2, y2 = target.long()\n",
        "        x_center, y_center, width, height = (x1 + x2) / 2, (y1 + y2) / 2, x2 - x1, y2 - y1\n",
        "\n",
        "        for i, grid_size in enumerate(gridSizes):\n",
        "            x_cell, y_cell = int(x_center * grid_size), int(y_center * grid_size)\n",
        "            anchor = 0\n",
        "\n",
        "            # Calculate the IoUs between the ground truth box and the anchors\n",
        "            ious = []\n",
        "            for a in anchors[i]:\n",
        "                iou = calculate_iou([x_center, y_center, width, height], a)\n",
        "                ious.append(iou)\n",
        "            anchor = torch.argmax(torch.tensor(ious))\n",
        "\n",
        "            try:\n",
        "                targetObj[i][batch_index, anchor, y_cell, x_cell, 0] = 1\n",
        "                targetBox[i][batch_index, anchor, y_cell, x_cell] = torch.tensor([x_center, y_center, width, height]) / grid_size\n",
        "            except Exception as e:\n",
        "                pass\n",
        "    return targetObj, targetBox\n",
        "\n",
        "class SignboardLoss(nn.Module):\n",
        "    def __init__(self, num_anchors=3):\n",
        "        super(SignboardLoss, self).__init__()\n",
        "        self.num_anchors = num_anchors\n",
        "\n",
        "    def forward(self, preds, targets):\n",
        "        objectLoss = torch.tensor(0.0, device=preds[0].device)\n",
        "        boxLoss = torch.tensor(0.0, device=preds[0].device)\n",
        "        batchSize = preds[0].size(0)\n",
        "        gridSizes = [pred.size(2) for pred in preds]\n",
        "        targetObjList, targetBoxList = TargetstoTensors(targets, batchSize, self.num_anchors, gridSizes, anchors)\n",
        "\n",
        "        for i, pred in enumerate(preds):\n",
        "            targetObj = targetObjList[i].to(pred.device)\n",
        "            targetBox = targetBoxList[i].to(pred.device)\n",
        "\n",
        "            objectLoss += nn.BCEWithLogitsLoss()(pred[..., 4:5], targetObj)\n",
        "            boxLoss += nn.SmoothL1Loss()(pred[..., :4], targetBox)  # Use Smooth L1 Loss instead of MSE\n",
        "\n",
        "        total_loss = objectLoss + boxLoss\n",
        "        return total_loss\n",
        "\n",
        "# ... other parts of the code\n",
        "def calculate_iou(box1, box2):\n",
        "    x1_center, y1_center, width1, height1 = box1\n",
        "    width2, height2 = box2\n",
        "\n",
        "    x1, y1 = x1_center - width1 / 2, y1_center - height1 / 2\n",
        "    x2, y2 = x1_center + width1 / 2, y1_center + height1 / 2\n",
        "    x3, y3 = x1_center - width2 / 2, y1_center - height2 / 2\n",
        "    x4, y4 = x1_center + width2 / 2, y1_center + height2 / 2\n",
        "\n",
        "    x_intersection = max(0, min(x2, x4) - max(x1, x3))\n",
        "    y_intersection = max(0, min(y2, y4) - max(y1, y3))\n",
        "\n",
        "    intersection_area = x_intersection * y_intersection\n",
        "    box1_area = width1 * height1\n",
        "    box2_area = width2 * height2\n",
        "    union_area = box1_area + box2_area - intersection_area\n",
        "\n",
        "    iou = intersection_area / union_area\n",
        "    return iou\n",
        "\n",
        "\n",
        "# NEED TO WORK ON THIS :\n",
        "\n",
        "# Example of values for 'anchors' variable\n",
        "anchors = [\n",
        "    # Anchors for grid size 13x13\n",
        "    [\n",
        "        [1.08, 1.19],\n",
        "        [3.42, 4.41],\n",
        "        [6.63, 11.38]\n",
        "    ],\n",
        "    # Anchors for grid size 26x26\n",
        "    [\n",
        "        [0.54, 0.58],\n",
        "        [1.55, 1.60],\n",
        "        [3.42, 3.41]\n",
        "    ],\n",
        "    # Anchors for grid size 52x52\n",
        "    [\n",
        "        [0.27, 0.32],\n",
        "        [0.62, 0.78],\n",
        "        [1.70, 1.97]\n",
        "    ]\n",
        "]\n",
        "# DISCLAIMER : You may need to adjust the anchor values to better suit your specific problem and dataset. The above example is just for illustration purposes. With these changes, your model should be better suited to handle the object detection task and potentially achieve higher accuracy and performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculating anchor values :\n",
        "\n",
        "\n",
        "# To find the best anchor values for your dataset, you can use a technique called k-means clustering. The idea is to cluster the aspect ratios of the ground truth bounding boxes in your dataset and use the centroids of the clusters as the anchor box dimensions. Here's how to do it:\n",
        "# Collect the ground truth bounding boxes from your dataset.\n",
        "# Normalize the bounding box dimensions (width and height) by the input image dimensions (width and height). This will give you the normalized aspect ratios of the bounding boxes.\n",
        "# Apply k-means clustering on the normalized aspect ratios to find the centroids. The number of clusters (k) should be equal to the number of anchor boxes you want to use. You can use a library like scikit-learn to perform k-means clustering.\n",
        "# The resulting centroids are your optimal anchor box dimensions.\n",
        "# Here's a sample code snippet to find the optimal anchor values using k-means clustering:\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def get_anchor_values(annotations, num_anchors):\n",
        "    aspect_ratios = []\n",
        "\n",
        "    for annotation in annotations:\n",
        "        x1, y1, x2, y2 = annotation\n",
        "        width, height = x2 - x1, y2 - y1\n",
        "        aspect_ratios.append([width, height])\n",
        "\n",
        "    aspect_ratios = np.array(aspect_ratios)\n",
        "\n",
        "    kmeans = KMeans(n_clusters=num_anchors, random_state=0)\n",
        "    kmeans.fit(aspect_ratios)\n",
        "\n",
        "    anchor_values = kmeans.cluster_centers_\n",
        "\n",
        "    return anchor_values.tolist()\n",
        "\n",
        "# In this function, annotations should be a list of bounding box coordinates (x1, y1, x2, y2), and num_anchors should be the total number of anchor boxes you want to use. This function will return a list of anchor values (width and height) that are best suited for your dataset.\n",
        "# Keep in mind that the anchor values provided by this method will be for a single grid size. If you're using YOLO with multiple grid sizes (e.g., 13x13, 26x26, 52x52), you might need to perform k-means clustering for each grid size separately or adapt the clustering process to handle multiple grid sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "def TrainEpoch(model, dataLoader, optimizer, lossFunction, device):\n",
        "    print(\"Training Epoch\")\n",
        "    model.train()\n",
        "    runningLoss = 0\n",
        "    dataLoaderLen = len(dataLoader)\n",
        "    for i, (inputs, targets) in enumerate(dataLoader):\n",
        "        # inputs = inputs.permute(2, 0, 1)\n",
        "        inputs = inputs.permute(0, 3, 1, 2)\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(True):\n",
        "            outputs = model(inputs)\n",
        "            loss = lossFunction(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        runningLoss += loss.item() * inputs.size(0)\n",
        "        if(((i*100)//dataLoaderLen) % 10 == 0):\n",
        "            print((i*100//dataLoaderLen), end=\"%,\")\n",
        "    epochLoss = runningLoss / dataLoaderLen\n",
        "    return model, epochLoss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "def ValidateEpoch(model, dataLoader, lossFunction, device):\n",
        "    print(\"Validating Epoch\")\n",
        "    model.eval()\n",
        "    runningLoss = 0\n",
        "    dataLoaderLen = len(dataLoader)\n",
        "    for i, (inputs, targets) in enumerate(dataLoader):\n",
        "        inputs = inputs.permute(2, 0, 1)\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        with torch.set_grad_enabled(False):\n",
        "            outputs = model(inputs)\n",
        "            loss = lossFunction(outputs, targets)\n",
        "        runningLoss += loss.item() * inputs.size(0)\n",
        "        if(((i*100)//dataLoaderLen) % 10 == 0):\n",
        "            print((i*100//dataLoaderLen), end=\"%,\")\n",
        "    epochLoss = runningLoss / dataLoaderLen\n",
        "    return epochLoss\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbVy1m6T3nB6"
      },
      "source": [
        "def TrainModel(model, trainDataLoader, valDataLoader, epochs, optimizer, scheduler, lossFunction, device):\n",
        "    for epoch in range(epochs):\n",
        "        startTime = time()\n",
        "        print(\"Epoch {}/{}:\".format(epoch+1, epochs))\n",
        "        startTime = time()\n",
        "        model, trainingEpochLoss = TrainEpoch(model, trainDataLoader, optimizer, lossFunction, device)\n",
        "        # validationEpochLoss = ValidateEpoch(model, valDataLoader, lossFunction, device)\n",
        "        # scheduler.step(validationEpochLoss)\n",
        "        scheduler.step(trainingEpochLoss)\n",
        "        endTime = time()\n",
        "        timeTaken = endTime - startTime\n",
        "        print()\n",
        "        print(\"Training Loss: {:.4f}\".format(trainingEpochLoss))\n",
        "        # print(\"validation Loss: {:.4f}\".format(validationEpochLoss))\n",
        "        print(\"Time taken: {}min, {}, secs\".format(timeTaken//60, int(timeTaken % 60)))\n",
        "    \n",
        "    print(\"Training complete.\")\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPT's SUGGESTIONS:\n",
        "\n",
        "# Add Tensorboard logging for better visualization of the training process.\n",
        "# Implement early stopping to avoid overfitting and save time.\n",
        "# Save the best model checkpoint based on the validation loss.\n",
        "# Use learning rate scheduling to adapt the learning rate during training.\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from time import time\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "def TrainEpoch(model, dataLoader, optimizer, lossFunction, device, writer, epoch):\n",
        "    print(\"Training Epoch\")\n",
        "    model.train()\n",
        "    runningLoss = 0\n",
        "    dataLoaderLen = len(dataLoader)\n",
        "    for i, (inputs, targets) in enumerate(dataLoader):\n",
        "        inputs = inputs.permute(0, 3, 1, 2)\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.set_grad_enabled(True):\n",
        "            outputs = model(inputs)\n",
        "            loss = lossFunction(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        runningLoss += loss.item() * inputs.size(0)\n",
        "        if(((i*100)//dataLoaderLen) % 10 == 0):\n",
        "            print((i*100//dataLoaderLen), end=\"%,\")\n",
        "    \n",
        "    epochLoss = runningLoss / dataLoaderLen\n",
        "    writer.add_scalar('Loss/train', epochLoss, epoch)\n",
        "    return model, epochLoss\n",
        "\n",
        "def ValidateEpoch(model, dataLoader, lossFunction, device, writer, epoch):\n",
        "    print(\"Validating Epoch\")\n",
        "    model.eval()\n",
        "    runningLoss = 0\n",
        "    dataLoaderLen = len(dataLoader)\n",
        "    for i, (inputs, targets) in enumerate(dataLoader):\n",
        "        inputs = inputs.permute(0, 3, 1, 2)\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        with torch.set_grad_enabled(False):\n",
        "            outputs = model(inputs)\n",
        "            loss = lossFunction(outputs, targets)\n",
        "        runningLoss += loss.item() * inputs.size(0)\n",
        "        if(((i*100)//dataLoaderLen) % 10 == 0):\n",
        "            print((i*100//dataLoaderLen), end=\"%,\")\n",
        "    \n",
        "    epochLoss = runningLoss / dataLoaderLen\n",
        "    writer.add_scalar('Loss/val', epochLoss, epoch)\n",
        "    return epochLoss\n",
        "\n",
        "def TrainModel(model, trainDataLoader, valDataLoader, epochs, optimizer, scheduler, lossFunction, device, log_dir='runs', early_stopping_patience=10):\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "    best_val_loss = np.inf\n",
        "    epochs_without_improvement = 0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        startTime = time()\n",
        "        print(\"Epoch {}/{}:\".format(epoch+1, epochs))\n",
        "        startTime = time()\n",
        "        model, trainingEpochLoss = TrainEpoch(model, trainDataLoader, optimizer, lossFunction, device, writer, epoch)\n",
        "        validationEpochLoss = ValidateEpoch(model, valDataLoader, lossFunction, device, writer, epoch)\n",
        "        scheduler.step(validationEpochLoss)\n",
        "        endTime = time()\n",
        "        timeTaken = endTime - startTime\n",
        "\n",
        "        if validationEpochLoss < best_val_loss:\n",
        "            best_val_loss = validationEpochLoss\n",
        "            best_model = model.state_dict()\n",
        "            torch.save(best_model, 'best_model.pt')\n",
        "            epochs_without_improvement = 0\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        print()\n",
        "        print(\"Training Loss: {:.4f}\".format(trainingEpochLoss))\n",
        "        print(\"validation Loss: {:.4f}\".format(validationEpochLoss))\n",
        "        print(\"Time taken: {}min, {} secs\".format(timeTaken // 60, int(timeTaken % 60)))\n",
        "\n",
        "        if epochs_without_improvement >= early_stopping_patience:\n",
        "            print(\"Early stopping triggered. No improvement in validation loss for {} epochs.\".format(early_stopping_patience))\n",
        "            break\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    model.load_state_dict(best_model)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Tensorboard logging is added to visualize the training and validation loss curves.\n",
        "# Early stopping is implemented to halt training when there's no improvement in the validation loss for a given number of epochs (controlled by early_stopping_patience).\n",
        "# The best model checkpoint (lowest validation loss) is saved to a file called 'best_model.pt'.\n",
        "# Learning rate scheduling is already present in the original code (with scheduler.step(validationEpochLoss)).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4adeyZX3nB6"
      },
      "source": [
        "batchSize = 32\n",
        "inputShape = (640, 640)\n",
        "epochs = 300\n",
        "numAnchors = 3\n",
        "yolo5Version = 'm'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_imN2T83nB6",
        "outputId": "5abee8fb-3512-4773-dcf1-e85a92c30ad0"
      },
      "source": [
        "print(\"Using {} device\".format(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(\"Downloading Weights of yolo5 Verion \", yolo5Version)\n",
        "# weightsURL = \"https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5{}.pt\".format(yolo5Version)\n",
        "# !wget {weightsURL}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z884-9l6bu--"
      },
      "source": [
        "images, annotations, numClasses = LoadDataSet(\"./DataSet1\")\n",
        "numClasses = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJAmoLT33nB6"
      },
      "source": [
        "X_train, X_val, y_train, y_val = PreProcessDataSet(\n",
        "    images, annotations, batchSize, inputShape)\n",
        "del images\n",
        "del annotations\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "try:\n",
        "    from google.colab.patches import cv2_imshow\n",
        "    cv2_imshow(X_train[10])\n",
        "except:\n",
        "    print(\"using Local\")\n",
        "    # cv2.imshow(\"Input Image\", image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IkU27xh3nB6"
      },
      "source": [
        "trainDataLoader, valDataLoader = CreateDataLoaders(\n",
        "    X_train, X_val, y_train, y_val, batchSize)\n",
        "del X_train\n",
        "del y_train\n",
        "del X_val\n",
        "del y_val\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJrRAHOrbu--",
        "outputId": "3cd16fbb-87f6-4a41-8663-76ace9abf5f1"
      },
      "source": [
        "yolov5Model = CreateYolov5Model(numClasses,yolo5Version)\n",
        "optimizer = optim.Adam(yolov5Model.parameters(), lr=0.001)\n",
        "yolov5LossFunction= SignboardLoss()\n",
        "yolov5Model = yolov5Model.to(device)\n",
        "yolov5LossFunction = yolov5LossFunction.to(device)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Atpoji13nB7",
        "outputId": "2b644c47-c751-4e4f-f561-5d9d6f549d94"
      },
      "source": [
        "trainedModel = TrainModel(yolov5Model, trainDataLoader,valDataLoader, epochs, optimizer, scheduler, yolov5LossFunction, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGDk5r7d3nB7"
      },
      "source": [
        "date = datetime.now()\n",
        "date = date.strftime(\"%m-%d-%H\")\n",
        "torch.save(trainedModel.state_dict(), 'yolov5Modelv2' + date +'.pth')\n",
        "shutil.copy('/content/yolov5Modelv2' + date +'.pth', '/content/drive/MyDrive/DL Project/Trained Models/yolov5Modelv2' + date +'.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPT'S SUGGESTIONS :\n",
        "# Removed the gc.collect() calls as they are not necessary in most cases. The garbage collector will automatically clean up unreferenced objects.\n",
        "# Added early_stopping_patience parameter to the TrainModel function to implement early stopping.\n",
        "# Added Tensorboard logging to the TrainModel function to visualize training and validation losses.\n",
        "\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "batchSize = 32\n",
        "inputShape = (640, 640)\n",
        "epochs = 300\n",
        "numAnchors = 3\n",
        "yolo5Version = 'm'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using {} device\".format(device))\n",
        "\n",
        "images, annotations, numClasses = LoadDataSet(\"./DataSet1\")\n",
        "numClasses = 1\n",
        "X_train, X_val, y_train, y_val = PreProcessDataSet(images, annotations, batchSize, inputShape)\n",
        "\n",
        "trainDataLoader, valDataLoader = CreateDataLoaders(X_train, X_val, y_train, y_val, batchSize)\n",
        "\n",
        "yolov5Model = CreateYolov5Model(numClasses, yolo5Version)\n",
        "optimizer = optim.Adam(yolov5Model.parameters(), lr=0.001)\n",
        "yolov5LossFunction = SignboardLoss()\n",
        "yolov5Model = yolov5Model.to(device)\n",
        "yolov5LossFunction = yolov5LossFunction.to(device)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
        "\n",
        "# Set up TensorBoard logging\n",
        "writer = SummaryWriter()\n",
        "\n",
        "trainedModel = TrainModel(yolov5Model, trainDataLoader, valDataLoader, epochs, optimizer, scheduler, yolov5LossFunction, device, writer, early_stopping_patience=20)\n",
        "\n",
        "date = datetime.now()\n",
        "date = date.strftime(\"%m-%d-%H\")\n",
        "torch.save(trainedModel.state_dict(), 'yolov5Modelv2' + date + '.pth')\n",
        "shutil.copy('/content/yolov5Modelv2' + date + '.pth', '/content/drive/MyDrive/DL Project/Trained Models/yolov5Modelv2' + date + '.pth')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
