{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39900,"status":"ok","timestamp":1682789221506,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"NLuliN1UcKJ7","outputId":"59b3f7a4-1a3f-44bd-b045-ebce5644a0ad"},"outputs":[],"source":["try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    import zipfile\n","    with zipfile.ZipFile('/content/drive/MyDrive/DL Project/YoloV5DataSet.zip', 'r') as zip_ref:\n","        zip_ref.extractall('./')\n","except:\n","    print(\"Using Local Machine\")\n","!git clone https://github.com/ultralytics/yolov5.git\n","!pip install -r yolov5/requirements.txt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":7188,"status":"ok","timestamp":1682789228691,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"nQdle6ECbu-5"},"outputs":[],"source":["# Include all packages\n","import gc\n","import cv2\n","import shutil\n","import random\n","import numpy as np\n","import pandas as pd\n","from time import time\n","from copy import deepcopy\n","\n","from datetime import datetime\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.optim import lr_scheduler\n","\n","\n","from yolov5.models.yolo import Model\n","from yolov5.utils.loss import ComputeLoss\n","import yolov5.val as validate\n","\n","from yolov5.utils.dataloaders import create_dataloader\n","from yolov5.utils.general import colorstr, intersect_dicts, check_amp, labels_to_class_weights, strip_optimizer, TQDM_BAR_FORMAT\n","from yolov5.utils.autobatch import check_train_batch_size\n","from yolov5.utils.torch_utils import EarlyStopping, ModelEMA, smart_optimizer, de_parallel\n","\n","import torchvision\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def CreateDataLoaders(trainPath, valPath,  imageSize, batchSize, gridSize, hyperParameter, numWorkers, options):\n","    trainLoader, dataset = create_dataloader(trainPath,\n","                                              imageSize,\n","                                              batchSize,\n","                                              gridSize,\n","                                              hyp=hyperParameter,\n","                                              workers=numWorkers,\n","                                              rect=options.rect,\n","                                              quad=options.quad,\n","                                              seed=options.seed,\n","                                              prefix=colorstr('train: '),\n","                                              image_weights=options.image_weights,\n","                                              cache=None if options.cache == 'val' else options.cache,\n","                                              augment=True,\n","                                              rank=-1,\n","                                              shuffle=True)\n","    valLoader = create_dataloader(valPath,\n","                                    imageSize,\n","                                    batchSize * 2,\n","                                    gridSize,\n","                                    hyp=hyperParameter,\n","                                    cache=None,\n","                                    rect=True,\n","                                    rank=-1,\n","                                    workers=numWorkers * 2,\n","                                    pad=0.5,\n","                                    prefix=colorstr('val: '))[0]\n","    \n","    labels = np.concatenate(dataset.labels, 0)\n","\n","    return trainLoader, valLoader, labels\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682789228692,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"AZJ8-pMgbu-9"},"outputs":[],"source":["def CreateYolov5Model(numClasses: int, version: str, device, weightsFilePath, configFilePath, hyperParameters ):\n","    if(weightsFilePath != None):\n","        ckpt = torch.load(weightsFilePath, map_location='cpu')  # load checkpoint to CPU to avoid CUDA memory leak\n","        model = Model(configFilePath, ch=3, nc=numClasses, anchors=hyperParameters.get('anchors')).to(device)  # create\n","        exclude = ['anchor'] \n","        csd = ckpt['model'].float().state_dict()  # checkpoint state_dict as FP32\n","        csd = intersect_dicts(csd, model.state_dict(), exclude=exclude)  # intersect\n","        model.load_state_dict(csd, strict=False)  # load\n","    else:\n","        model = Model(configFilePath, ch=3, nc=numClasses, anchors=hyperParameters.get('anchors')).to(device)  # create\n","    amp = check_amp(model)  # check AMP\n","\n","    return model, amp\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def AddModelAttributes(model, hyperParameters, numClasses, imageSize, options, dataset, device, classNames):\n","    nl = de_parallel(model).model[-1].nl\n","    hyperParameters['box'] *= 3 / nl  \n","    hyperParameters['cls'] *= numClasses / 80 * 3 / nl  \n","    hyperParameters['obj'] *= (imageSize / 640) ** 2 * 3 / nl  \n","    hyperParameters['label_smoothing'] = options.label_smoothing\n","    model.nc = numClasses \n","    model.hyp = hyperParameters\n","    model.class_weights = labels_to_class_weights(dataset.labels, numClasses).to(device) * numClasses  # attach class weights\n","    model.names = classNames\n","    return model, hyperParameters"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def GetConfig(model, imageSize, amp, hyperParameters, options, epochs, nominalBatchSize = 64):\n","    gridSize = max(int(model.stride.max()), 32)  # grid size (max stride)\n","    batchSize = check_train_batch_size(model, imageSize, amp)\n","    accumulate = max(round(nominalBatchSize / batchSize), 1)\n","    hyperParameters['weight_decay'] *= batchSize * accumulate / nominalBatchSize \n","    optimizer = smart_optimizer(model, options.optimizer, hyperParameters['lr0'], hyperParameters['momentum'], hyperParameters['weight_decay'])\n","\n","    # Scheduler\n","    lf = lambda x: (1 - x / epochs) * (1.0 - hyperParameters['lrf']) + hyperParameters['lrf'] \n","    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lf) \n","\n","    # EMA\n","    ema = ModelEMA(model)\n","\n","\n","    return gridSize, batchSize,accumulate, optimizer, scheduler, ema\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def TrainModel(model, trainLoader,numEpochs, ema, batchSize, device,optimizer,  hyperParameters, lf, numClasses, options, scheduler, amp, patience=100, nominalBatchSize= 64 ):\n","    numBatchs = len(trainLoader) \n","    numWarmup = max(round(hyperParameters['warmup_epochs'] * numBatchs), 100) \n","    lastOptStep = -1\n","    mAPc = np.zeros(numClasses)\n","    results = (0, 0, 0, 0, 0, 0, 0)\n","    scheduler.last_epoch = - 1\n","    scaler = torch.cuda.amp.GradScaler(enabled=amp)\n","    earlyStopper, earlyStop = EarlyStopping(patience=patience), False\n","    computeLoss = ComputeLoss(model)  # init loss class\n","    for epoch in range(numEpochs):\n","        model.train()\n","        meamLoss = torch.zeros(3, device=device)  # mean losses\n","        pbar = tqdm(pbar, total=numBatchs, bar_format=TQDM_BAR_FORMAT)\n","        optimizer.zero_grad()\n","        for i, (imgs, targets, paths, _) in pbar:  \n","            numIntergrations = i + numBatchs * epoch  \n","            imgs = imgs.to(device, non_blocking=True).float() / 255 \n","            \n","            # Warmup\n","            if numIntergrations <= numWarmup:\n","                xi = [0, numWarmup]  # x interp\n","                # compute_loss.gr = np.interp(ni, xi, [0.0, 1.0])  # iou loss ratio (obj_loss = 1.0 or iou)\n","                accumulate = max(1, np.interp(numIntergrations, xi, [1, nominalBatchSize / batchSize]).round())\n","                for j, x in enumerate(optimizer.param_groups):\n","                    # bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0\n","                    x['lr'] = np.interp(numIntergrations, xi, [hyperParameters['warmup_bias_lr'] if j == 0 else 0.0, x['initial_lr'] * lf(epoch)])\n","                    if 'momentum' in x:\n","                        x['momentum'] = np.interp(numIntergrations, xi, [hyperParameters['warmup_momentum'], hyperParameters['momentum']])\n","            \n","            # Multi-scale\n","            # if opt.multi_scale:\n","            #     sz = random.randrange(int(imgsz * 0.5), int(imgsz * 1.5) + gs) // gs * gs  # size\n","            #     sf = sz / max(imgs.shape[2:])  # scale factor\n","            #     if sf != 1:\n","            #         ns = [math.ceil(x * sf / gs) * gs for x in imgs.shape[2:]]  # new shape (stretched to gs-multiple)\n","            #         imgs = nn.functional.interpolate(imgs, size=ns, mode='bilinear', align_corners=False)\n","            \n","            # Forward\n","            with torch.cuda.amp.autocast(amp):\n","                pred = model(imgs)\n","                loss, lossItems = computeLoss(pred, targets.to(device))\n","                # if options.quad:\n","                #     loss *= 4.\n","            \n","             # Backward\n","            scaler.scale(loss).backward()\n","\n","            # Optimize - https://pytorch.org/docs/master/notes/amp_examples.html\n","            if (numIntergrations - lastOptStep >= accumulate):\n","                scaler.unscale_(optimizer)  # unscale gradients\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)  # clip gradients\n","                scaler.step(optimizer)  # optimizer.step\n","                scaler.update()\n","                optimizer.zero_grad()\n","                if( ema):\n","                    ema.update(model)\n","                lastOptStep = numIntergrations\n","        # Scheduler\n","        lr = [x['lr'] for x in optimizer.param_groups]  # for loggers\n","        scheduler.step()\n","        # if RANK in {-1, 0}:\n","        #     # mAP\n","        #     callbacks.run('on_train_epoch_end', epoch=epoch)\n","        #     ema.update_attr(model, include=['yaml', 'nc', 'hyp', 'names', 'stride', 'class_weights'])\n","        #     final_epoch = (epoch + 1 == epochs) or stopper.possible_stop\n","        #     if not noval or final_epoch:  # Calculate mAP\n","        #         results, maps, _ = validate.run(data_dict,\n","        #                                         batch_size=batch_size // WORLD_SIZE * 2,\n","        #                                         imgsz=imgsz,\n","        #                                         half=amp,\n","        #                                         model=ema.ema,\n","        #                                         single_cls=single_cls,\n","        #                                         dataloader=val_loader,\n","        #                                         save_dir=save_dir,\n","        #                                         plots=False,\n","        #                                         callbacks=callbacks,\n","        #                                         compute_loss=compute_loss)\n","\n","        #     # Update best mAP\n","        #     fi = fitness(np.array(results).reshape(1, -1))  # weighted combination of [P, R, mAP@.5, mAP@.5-.95]\n","        #     stop = stopper(epoch=epoch, fitness=fi)  # early stop check\n","        #     if fi > best_fitness:\n","        #         best_fitness = fi\n","        #     log_vals = list(mloss) + list(results) + lr\n","        #     callbacks.run('on_fit_epoch_end', log_vals, epoch, best_fitness, fi)\n","\n","        #     # Save model\n","        #     if (not nosave) or (final_epoch and not evolve):  # if save\n","        #         ckpt = {\n","        #             'epoch': epoch,\n","        #             'best_fitness': best_fitness,\n","        #             'model': deepcopy(de_parallel(model)).half(),\n","        #             'ema': deepcopy(ema.ema).half(),\n","        #             'updates': ema.updates,\n","        #             'optimizer': optimizer.state_dict(),\n","        #             'opt': vars(opt),\n","        #             'git': GIT_INFO,  # {remote, branch, commit} if a git repo\n","        #             'date': datetime.now().isoformat()}\n","\n","        #         # Save last, best and delete\n","        #         torch.save(ckpt, last)\n","        #         if best_fitness == fi:\n","        #             torch.save(ckpt, best)\n","        #         if opt.save_period > 0 and epoch % opt.save_period == 0:\n","        #             torch.save(ckpt, w / f'epoch{epoch}.pt')\n","        #         del ckpt\n","        #         callbacks.run('on_model_save', last, epoch, final_epoch, best_fitness, fi)\n","\n","        # # EarlyStopping\n","        # if RANK != -1:  # if DDP training\n","        #     broadcast_list = [stop if RANK == 0 else None]\n","        #     dist.broadcast_object_list(broadcast_list, 0)  # broadcast 'stop' to all ranks\n","        #     if RANK != 0:\n","        #         stop = broadcast_list[0]\n","        # if stop:\n","        #     break  # must break all DDP ranks\n","    #     last = 'last.pt'\n","    #     best = 'best.pt'\n","    #     for f in last, best:\n","    #         if f.exists():\n","    #             strip_optimizer(f)  # strip optimizers\n","    #             if f is best:\n","    #                 results, _, _ = validate.run(\n","    #                     data_dict,\n","    #                     batch_size=batch_size // WORLD_SIZE * 2,\n","    #                     imgsz=imgsz,\n","    #                     model=attempt_load(f, device).half(),\n","    #                     iou_thres=0.65 if is_coco else 0.60,  # best pycocotools at iou 0.65\n","    #                     single_cls=single_cls,\n","    #                     dataloader=val_loader,\n","    #                     save_dir=save_dir,\n","    #                     save_json=is_coco,\n","    #                     verbose=True,\n","    #                     plots=plots,\n","    #                     callbacks=callbacks,\n","    #                     compute_loss=compute_loss)  # val best model with plots\n","\n","    # torch.cuda.empty_cache()\n","    # return results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1682789229546,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"n4adeyZX3nB6"},"outputs":[],"source":["batchSize = 32\n","inputShape = 640\n","epochs = 100\n","numAnchors = 3\n","yolo5Version = 's'\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","numClasses = 10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1682789229546,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"A_imN2T83nB6","outputId":"04adf1cd-c96c-447f-a899-72b212b134a5"},"outputs":[],"source":["print(\"Using {} device\".format(device))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1430,"status":"ok","timestamp":1682789230971,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"h5e6cK8HcKJ_","outputId":"3329e4df-4a17-44f1-98b8-fe0075260444"},"outputs":[],"source":["print(\"Downloading Weights of yolo5 Verion \", yolo5Version)\n","weightsURL = \"https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5{}.pt\".format(\n","    yolo5Version)\n","!wget {weightsURL}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6793,"status":"ok","timestamp":1682789246205,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"WJrRAHOrbu--","outputId":"f0c5a931-ea45-481e-de99-c04bd8c07aac"},"outputs":[],"source":["yolov5Model = CreateYolov5Model(numClasses, yolo5Version, device)\n","optimizer = optim.Adam(yolov5Model.parameters(), lr=0.01)\n","\n","yolov5Model = yolov5Model.to(device)\n","scheduler = ReduceLROnPlateau(\n","    optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":52200,"status":"error","timestamp":1682789545288,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"9Atpoji13nB7","outputId":"4213798b-b682-47c5-a7d9-a5e52f8ef32c"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"aborted","timestamp":1682789545289,"user":{"displayName":"anudeep kalitar","userId":"01854914863512508282"},"user_tz":360},"id":"rGDk5r7d3nB7"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
