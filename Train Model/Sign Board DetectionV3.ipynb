{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NLuliN1UcKJ7","outputId":"12abff3b-2fa6-4578-c1eb-62178ad1b173"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","Cloning into 'yolov5'...\n","remote: Enumerating objects: 15598, done.\u001b[K\n","remote: Counting objects: 100% (205/205), done.\u001b[K\n","remote: Compressing objects: 100% (149/149), done.\u001b[K\n"]}],"source":["try:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    import zipfile\n","    with zipfile.ZipFile('/content/drive/MyDrive/DL Project/DataSet1.zip', 'r') as zip_ref:\n","        zip_ref.extractall('./DataSet1')\n","except:\n","    print(\"Using Local Machine\")\n","!git clone https://github.com/ultralytics/yolov5.git\n","!pip install -r yolov5/requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Gi1JVb63nB1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQdle6ECbu-5"},"outputs":[],"source":["# Include all packages\n","import gc\n","import cv2\n","import shutil\n","import random\n","import numpy as np\n","import pandas as pd\n","from time import time\n","from copy import deepcopy\n","\n","from datetime import datetime\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","\n","from yolov5.models.yolo import Model\n","from sklearn.model_selection import train_test_split\n","\n","from pycocotools.coco import COCO\n","from pycocotools.cocoeval import COCOeval\n","\n","import torchvision\n","from torch.optim.lr_scheduler import ReduceLROnPlateau"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SxOLsWl2cKJ8"},"outputs":[],"source":["def CannyEdge(capturedImage):\n","    grayScale = cv2.cvtColor(capturedImage, cv2.COLOR_BGR2GRAY)\n","    constrastKernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5) )\n","    topHat = cv2.morphologyEx(grayScale, cv2.MORPH_TOPHAT, constrastKernel)\n","    blackHat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, constrastKernel)\n","    grayScale = grayScale + topHat - blackHat\n","    bilateralFilter = cv2.bilateralFilter(grayScale, 11, 17, 17)\n","    imageMedian = np.median(capturedImage)\n","    lowerThreshold = max(0, (0.7 * imageMedian))\n","    upperThreshold = min(255, (0.7 * imageMedian))\n","    cannyEdgeImage = cv2.Canny(bilateralFilter, lowerThreshold, upperThreshold)\n","    cannyEdgeImage = cv2.bitwise_not(cannyEdgeImage)\n","    cannyEdgeImage = cv2.cvtColor(cannyEdgeImage, cv2.COLOR_GRAY2BGR)\n","    return cannyEdgeImage"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xd2tU8GB3nB3"},"outputs":[],"source":["def ResizeImage(image: np.ndarray, x1: int, y1: int, x2: int, y2: int, newWidth: int, newHeight: int) -> tuple:\n","    originalHeight, originalWidth = image.shape[:2]\n","    widthScale = newWidth / originalWidth\n","    heightScale = newHeight / originalHeight\n","    resizedImage = cv2.resize(\n","        image, (newWidth, newHeight), interpolation=cv2.INTER_LINEAR)\n","    x1New, y1New = int(x1 * widthScale), int(y1 * heightScale)\n","    x2New, y2New = int(x2 * widthScale), int(y2 * heightScale)\n","    return resizedImage, x1New, y1New, x2New, y2New\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9iMxCddibu-7"},"outputs":[],"source":["def LoadDataSet(dataSetFolderPath: str) -> tuple:\n","    images = []\n","    annotations = []\n","    annotationsFilePath = dataSetFolderPath+\"/annotations.csv\"\n","    annotationsDataFrame = pd.read_csv(annotationsFilePath, sep=\",\")\n","    uniqueSigns = annotationsDataFrame['class'].unique().tolist()\n","    for index, row in annotationsDataFrame[1:].iterrows():\n","        image = cv2.imread(dataSetFolderPath+\"/\"+row[0])\n","        images.append(image)\n","        annotations.append(\n","            [uniqueSigns.index(row[5]), row[1], row[2], row[3], row[4]])\n","\n","    del annotationsDataFrame\n","\n","    return images, annotations, len(uniqueSigns)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"87wkTQy53nB4"},"outputs":[],"source":["def PreProcessDataSet(images: list, annotations: list, numClasses: int, resize: tuple) -> tuple:\n","    resizedImages = [[] for i in range(numClasses)]\n","    newAnnotations = [[] for i in range(numClasses)]\n","    for i, image in enumerate(images):\n","        [classIndex, x1, y1, x2, y2] = annotations[i]\n","        resizedImage, x1New, y1New, x2New, y2New = ResizeImage(\n","            image, x1, y1, x2, y2, resize[0], resize[1])\n","        resizedImages[classIndex].append(resizedImage)\n","        newAnnotations[classIndex].append(\n","            [x1New, y1New, x2New, y2New])\n","    \n","    return resizedImages, newAnnotations\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68sCzcEAbu-7"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, data, transform=None):\n","        self.data = data\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        inputData, label = self.data[idx]\n","\n","        if self.transform:\n","            inputData = self.transform(inputData)\n","        inputData = torch.from_numpy(inputData).float()\n","        label = torch.tensor(label).float()\n","        return inputData, label\n","\n","def CreateDataLoaders(X_train, X_val, y_train, y_val, batchSize):\n","    trainDataSet = []\n","    valDataSet = []\n","    for i in range(len(X_train)):\n","        trainDataSet.append((X_train[i], y_train[i]))\n","\n","    for i in range(len(X_val)):\n","        valDataSet.append((X_val[i], y_val[i]))\n","\n","    trainDataSet = CustomDataset(trainDataSet)\n","    valDataSet = CustomDataset(valDataSet)\n","    trainDataLoader = DataLoader(\n","        trainDataSet, batch_size=batchSize, shuffle=True, num_workers=4)\n","    valDataLoader = DataLoader(\n","        valDataSet, batch_size=batchSize, shuffle=False, num_workers=4)\n","\n","    return trainDataLoader, valDataLoader\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"owhzzQF61a0L"},"outputs":[],"source":["\n","def TargetstoTensors(targets, batchSize, numAnchors, gridSizes):\n","    targetObj = []\n","    targetBox = []\n","    for grid_size in gridSizes:\n","        targetObj.append(torch.zeros((batchSize, numAnchors, grid_size, grid_size, 1)))\n","        targetBox.append(torch.zeros((batchSize, numAnchors, grid_size, grid_size, 4)))\n","\n","    for batch_index, target in enumerate(targets):\n","        x1, y1, x2, y2 = target.long()\n","        x_center, y_center, width, height = (x1 + x2) / 2, (y1 + y2) / 2, x2 - x1, y2 - y1\n","\n","        for i, grid_size in enumerate(gridSizes):\n","            x_cell, y_cell = int(x_center * grid_size), int(y_center * grid_size)\n","            anchor = 0\n","            try:\n","                targetObj[i][batch_index, anchor, y_cell, x_cell, 0] = 1\n","                targetBox[i][batch_index, anchor, y_cell, x_cell] = torch.tensor([x_center, y_center, width, height])\n","            except Exception as e:\n","                pass\n","    return targetObj, targetBox"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bCXFHrH5zfQx"},"outputs":[],"source":["\n","class SignboardLoss(nn.Module):\n","    def __init__(self, num_anchors=3):\n","        super(SignboardLoss, self).__init__()\n","        self.num_anchors = num_anchors\n","\n","    def forward(self, preds, targets):\n","        objectLoss = torch.tensor(0.0, device=preds[0].device)\n","        boxLoss = torch.tensor(0.0, device=preds[0].device)\n","        batchSize = preds[0].size(0)\n","        gridSizes = [pred.size(2) for pred in preds]\n","        targetObjList, targetBoxList = TargetstoTensors(targets, batchSize, self.num_anchors, gridSizes)\n","\n","        for i, pred in enumerate(preds):\n","            targetObj = targetObjList[i].to(pred.device)\n","            targetBox = targetBoxList[i].to(pred.device)\n","\n","            objectLoss += nn.BCEWithLogitsLoss()(pred[..., 4:5], targetObj)\n","            boxLoss += nn.MSELoss()(pred[..., :4], targetBox)\n","\n","        total_loss = objectLoss + boxLoss\n","        return total_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AZJ8-pMgbu-9"},"outputs":[],"source":["def CreateYolov5Model(numClasses: int, version: str):\n","    congfigFile = \"yolov5/models/yolov5{}.yaml\".format(version)\n","    model = Model(congfigFile, ch=3, nc=numClasses)\n","    ckpt = torch.load(f'yolov5{version}.pt', map_location=device)\n","    ckpt_model_dict = ckpt['model'].state_dict()\n","    compatible_weights = {k: v for k, v in ckpt_model_dict.items() if k in model.state_dict() and model.state_dict()[k].shape == v.shape}\n","    model.load_state_dict(compatible_weights, strict=False)\n","    model.hyp = ckpt['model'].hyp\n","    return model\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWVv6VcgcKJ-"},"outputs":[],"source":["def TrainEpoch(model, dataLoader, optimizer, lossFunction, device):\n","    # print(\"Training Epoch\")\n","    model.train()\n","    runningLoss = 0\n","    dataLoaderLen = len(dataLoader)\n","    for i, (inputs, targets) in enumerate(dataLoader):\n","        # inputs = inputs.permute(2, 0, 1)\n","        inputs = inputs.permute(0, 3, 1, 2)\n","        inputs = inputs.to(device)\n","        targets = targets.to(device)\n","        optimizer.zero_grad()\n","        with torch.set_grad_enabled(True):\n","            outputs = model(inputs)\n","            loss = lossFunction(outputs, targets)\n","            loss.backward()\n","            optimizer.step()\n","\n","        runningLoss += loss.item() * inputs.size(0)\n","        # if(((i*100)//dataLoaderLen) % 10 == 0):\n","        #     print((i*100//dataLoaderLen), end=\"%,\")\n","    epochLoss = runningLoss / dataLoaderLen\n","    return model, epochLoss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GCtFNZeAcKJ-"},"outputs":[],"source":["def DetectImage(model, inputs, device, conf_thres=0.2, iou_thres=0.5):\n","    model.eval()\n","    inputs = torch.tensor(inputs, dtype=torch.float32)\n","    inputs = inputs.unsqueeze(0)\n","    inputs = inputs.permute(0, 3, 1, 2)\n","    inputs = inputs.to(device)\n","    conf_thres = torch.tensor(conf_thres)\n","    with torch.no_grad():\n","        output = model(inputs)\n","        # max_conf_obj_idx = torch.argmax(output[0][..., 4:5], dim=1)\n","        # output = output[0][torch.arange(output[0].size(0)), max_conf_obj_idx]\n","        # output = torchvision.ops.nms(output, conf_thres, iou_thres)\n","        # max_conf_obj_idx = torch.argmax(output[0][..., 4:5], dim=1)\n","        # output = output[0][torch.arange(output[0].size(0)), max_conf_obj_idx]\n","        output = output[0]\n","        box_coordinates = output[..., :4].view(-1, 4)\n","        confidence_scores = output[..., 4].view(-1)\n","        nms_indices = torchvision.ops.nms(box_coordinates, confidence_scores, iou_thres)\n","        output = output.view(-1, output.shape[-1])[nms_indices]\n","    # Remove the batch dimension\n","    output = output.squeeze(0)\n","    return output\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vmu96jtn_Xv5"},"outputs":[],"source":["\n","def EvaluateModel( yolov5Model,X_val: list, y_val: list):\n","    randInt = random.randint(0,len(X_val))\n","    image = X_val[randInt]\n","    print(len(X_val))\n","    image1 = deepcopy(image)\n","    predictions = DetectImage(yolov5Model, image, device)\n","    [a1,b1,a2,b2] = y_val[randInt]\n","    bBoxs = [[a1,b1,a2,b2]]\n","    machingbBoxes = []\n","    for pred in predictions:\n","        x1, y1, x2, y2, m1,m2 = pred[:6]\n","        m1,m2, x1, y1, x2, y2= int(m1), int(m2),int(x1), int(y1), int(x2), int(y2)\n","    if(a1 == x1 or a2 == x2 or b1 == y1 or b2 == y2 ):\n","        if(((x1-x2) >= 17 and (x1-x2) <= 32) and ((y1-y2) >= 31 and (y1-y2)<= 56) ):\n","            machingbBoxes.append([x1, y1, x2,y2])\n","      \n","    if(((x1-x2) >= 17 and (x1-x2) <= 32) and ((y1-y2) >= 31 and (y1-y2)<= 56) ):\n","        bBoxs.append([x1, y1, x2, y2])\n","    print(\"No. Objects detected:\" ,len(bBoxs) )\n","    print(\"No. Matching Objects detected:\" ,len(machingbBoxes) )\n","    cv2.rectangle(image, (a1, b1), (a2, b2), (0,255,0), 2)\n","    for bBox in bBoxs[1:]:\n","        # print(bBox)\n","        [x1, y1, x2, y2] = bBox\n","        cv2.rectangle(image, (x1, y1), (x2, y2), (0,0,255), 2)\n","    for bBox in machingbBoxes:\n","        [x1, y1, x2, y2] = bBox\n","        cv2.rectangle(image, (x1, y1), (x2, y2), (255,0,0), 2)\n","    try:\n","        from google.colab.patches import cv2_imshow\n","        cv2_imshow(image1)\n","    except:\n","        print(\"using Local\")\n","        cv2.imshow(\"Input Image\", image1)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YbVy1m6T3nB6"},"outputs":[],"source":["def TrainModel(model, trainDataLoader, valDataLoader, epochs, optimizer, scheduler, lossFunction, device):\n","    startTime = time()\n","    for epoch in range(epochs):\n","        # print(\"Epoch {}/{}:\".format(epoch+1, epochs))\n","        model, trainingEpochLoss = TrainEpoch(model, trainDataLoader, optimizer, lossFunction, device)\n","        scheduler.step(trainingEpochLoss)\n","        if(((epoch*100)/epochs) % 10 == 0):\n","            endTime = time()\n","            timeTaken = endTime - startTime\n","            print((epoch*100//epochs), end=\"%,\")\n","            print(\"Training Loss: {:.4f}\".format(trainingEpochLoss), end=\",\")\n","            print(\"Time taken: {}min, {}, secs\".format(timeTaken//60, int(timeTaken % 60)))\n","            startTime = time()\n","    \n","    print()\n","    \n","    \n","    print(\"Training complete.\")\n","    return model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n4adeyZX3nB6"},"outputs":[],"source":["batchSize = 32\n","inputShape = (640, 640)\n","epochs = 300\n","numAnchors = 3\n","yolo5Version = 'm'\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A_imN2T83nB6"},"outputs":[],"source":["print(\"Using {} device\".format(device))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h5e6cK8HcKJ_"},"outputs":[],"source":["print(\"Downloading Weights of yolo5 Verion \", yolo5Version)\n","weightsURL = \"https://github.com/ultralytics/yolov5/releases/download/v5.0/yolov5{}.pt\".format(yolo5Version)\n","!wget {weightsURL}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z884-9l6bu--"},"outputs":[],"source":["images, annotations, numClasses = LoadDataSet(\"./DataSet1\")\n","print(numClasses)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cJAmoLT33nB6"},"outputs":[],"source":["# X_train, X_val, y_train, y_val = PreProcessDataSet(\n","#     images, annotations, batchSize, inputShape)\n","X_train, y_train = PreProcessDataSet(\n","    images, annotations, numClasses, inputShape)\n","X_val = [[] for i in range(numClasses)]\n","y_val = [[] for i in range(numClasses)]\n","del images\n","del annotations\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VM_TDrZjcKJ_"},"outputs":[],"source":["# try:\n","#     from google.colab.patches import cv2_imshow\n","#     cv2_imshow(X_train[10])\n","# except:\n","#     print(\"using Local\")\n","#     # cv2.imshow(\"Input Image\", image)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WJrRAHOrbu--"},"outputs":[],"source":["yolov5Model = CreateYolov5Model(1,yolo5Version)\n","optimizer = optim.Adam(yolov5Model.parameters(), lr=0.01)\n","yolov5LossFunction= SignboardLoss()\n","yolov5Model = yolov5Model.to(device)\n","yolov5LossFunction = yolov5LossFunction.to(device)\n","scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Atpoji13nB7"},"outputs":[],"source":["for i in range(numClasses):\n","    print( \"training class\", i, \"with number of samples:\", len(X_train[i]))\n","    trainDataLoader, valDataLoader = CreateDataLoaders(\n","        X_train[i], X_val[i], y_train[i], y_val[i], batchSize)\n","    epochs = (900//len(X_train[i]))*100\n","    print(\"epochs\", epochs)\n","    yolov5Model = TrainModel(yolov5Model, trainDataLoader,valDataLoader, epochs, optimizer, scheduler, yolov5LossFunction, device)\n","    EvaluateModel( yolov5Model,X_train[i], y_train[i])\n","    date = datetime.now()\n","    date = date.strftime(\"%m-%d-%H\")\n","    torch.save(yolov5Model.state_dict(), 'yolov5SingleModelv2' + date +'-class-'+str(i)+'.pth')\n","    shutil.copy('/content/yolov5SingleModelv2' + date +'-class-'+str(i)+'.pth', '/content/drive/MyDrive/DL Project/Trained Models/yolov5SingleModelv2' + date +'-class-'+str(i)+'.pth')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rGDk5r7d3nB7"},"outputs":[],"source":["date = datetime.now()\n","date = date.strftime(\"%m-%d-%H\")\n","torch.save(yolov5Model.state_dict(), 'yolov5SingleModelv2' + date +'.pth')\n","shutil.copy('/content/yolov5SingleModelv2' + date +'.pth', '/content/drive/MyDrive/DL Project/Trained Models/yolov5SingleModelv2' + date +'.pth')\n"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
